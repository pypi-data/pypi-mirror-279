# This file was auto-generated by Fern from our API Definition.

import json
import typing
import urllib.parse
from json.decoder import JSONDecodeError

import httpx_sse

from ..core.api_error import ApiError
from ..core.client_wrapper import AsyncClientWrapper, SyncClientWrapper
from ..core.jsonable_encoder import jsonable_encoder
from ..core.pydantic_utilities import pydantic_v1
from ..core.remove_none_from_dict import remove_none_from_dict
from ..core.request_options import RequestOptions
from .errors.internal_server_error import InternalServerError
from .errors.unprocessable_entity_error import UnprocessableEntityError
from .types.chat_completion_chunk import ChatCompletionChunk
from .types.chat_completion_request_ext import ChatCompletionRequestExt
from .types.chat_completion_response import ChatCompletionResponse
from .types.chat_completion_response_format import ChatCompletionResponseFormat
from .types.chat_message import ChatMessage
from .types.completion_response import CompletionResponse
from .types.error_response import ErrorResponse
from .types.http_validation_error import HttpValidationError
from .types.prompt import Prompt
from .types.stop import Stop
from .types.stream_options import StreamOptions

# this is used as the default value for optional parameters
OMIT = typing.cast(typing.Any, ...)


class TextGenClient:
    def __init__(self, *, client_wrapper: SyncClientWrapper):
        self._client_wrapper = client_wrapper

    def create_chat_completion_stream(
        self,
        *,
        messages: typing.Sequence[ChatMessage],
        model: str,
        frequency_penalty: typing.Optional[float] = OMIT,
        ignore_eos: typing.Optional[bool] = OMIT,
        logit_bias: typing.Optional[typing.Dict[str, typing.Optional[float]]] = OMIT,
        loglikelihood: typing.Optional[bool] = OMIT,
        logprobs: typing.Optional[bool] = OMIT,
        max_tokens: typing.Optional[int] = OMIT,
        n: typing.Optional[int] = OMIT,
        octoai: typing.Optional[ChatCompletionRequestExt] = OMIT,
        presence_penalty: typing.Optional[float] = OMIT,
        repetition_penalty: typing.Optional[float] = OMIT,
        response_format: typing.Optional[ChatCompletionResponseFormat] = OMIT,
        stop: typing.Optional[Stop] = OMIT,
        stream_options: typing.Optional[StreamOptions] = OMIT,
        temperature: typing.Optional[float] = OMIT,
        top_logprobs: typing.Optional[int] = OMIT,
        top_p: typing.Optional[float] = OMIT,
        user: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.Iterator[ChatCompletionChunk]:
        """
        Create a Chat Completion.

        Parameters
        ----------
        messages : typing.Sequence[ChatMessage]
            A list of messages comprising the conversation so far.

        model : str
            The identifier of the model to use.Can be a shared tenancy or custom model identifier.

        frequency_penalty : typing.Optional[float]
            Penalizes new tokens based on their frequency in the generated text so far.

        ignore_eos : typing.Optional[bool]

        logit_bias : typing.Optional[typing.Dict[str, typing.Optional[float]]]

        loglikelihood : typing.Optional[bool]

        logprobs : typing.Optional[bool]

        max_tokens : typing.Optional[int]
            Maximum number of tokens to generate per output sequence.

        n : typing.Optional[int]

        octoai : typing.Optional[ChatCompletionRequestExt]

        presence_penalty : typing.Optional[float]
            Penalizes new tokens based on whether they appear in the generated text so far

        repetition_penalty : typing.Optional[float]
            Controls the likelihood of the model generating repeated texts

        response_format : typing.Optional[ChatCompletionResponseFormat]

        stop : typing.Optional[Stop]

        stream_options : typing.Optional[StreamOptions]

        temperature : typing.Optional[float]
            Controls the randomness of the sampling.

        top_logprobs : typing.Optional[int]

        top_p : typing.Optional[float]
            Controls the cumulative probability of the top tokens to consider.

        user : typing.Optional[str]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Yields
        ------
        typing.Iterator[ChatCompletionChunk]


        Examples
        --------
        from octoai.client import OctoAI
        from octoai.text_gen import (
            ChatCompletionRequestExt,
            ChatCompletionRequestExtVllm,
            ChatCompletionResponseFormat,
            ChatMessage,
            StreamOptions,
        )

        client = OctoAI(
            api_key="YOUR_API_KEY",
        )
        client.text_gen.create_chat_completion_stream(
            frequency_penalty=1.1,
            ignore_eos=True,
            logit_bias={"string": {"key": "value"}},
            loglikelihood=True,
            logprobs=True,
            max_tokens=1,
            messages=[
                ChatMessage(
                    content="string",
                    role="string",
                )
            ],
            model="string",
            n=1,
            octoai=ChatCompletionRequestExt(
                vllm=ChatCompletionRequestExtVllm(),
            ),
            presence_penalty=1.1,
            repetition_penalty=1.1,
            response_format=ChatCompletionResponseFormat(
                schema={"string": {"key": "value"}},
                type="string",
            ),
            stop="string",
            stream_options=StreamOptions(
                include_usage=True,
            ),
            temperature=1.1,
            top_logprobs=1,
            top_p=1.1,
            user="string",
        )
        """
        _request: typing.Dict[str, typing.Any] = {"messages": messages, "model": model, "stream": True}
        if frequency_penalty is not OMIT:
            _request["frequency_penalty"] = frequency_penalty
        if ignore_eos is not OMIT:
            _request["ignore_eos"] = ignore_eos
        if logit_bias is not OMIT:
            _request["logit_bias"] = logit_bias
        if loglikelihood is not OMIT:
            _request["loglikelihood"] = loglikelihood
        if logprobs is not OMIT:
            _request["logprobs"] = logprobs
        if max_tokens is not OMIT:
            _request["max_tokens"] = max_tokens
        if n is not OMIT:
            _request["n"] = n
        if octoai is not OMIT:
            _request["octoai"] = octoai
        if presence_penalty is not OMIT:
            _request["presence_penalty"] = presence_penalty
        if repetition_penalty is not OMIT:
            _request["repetition_penalty"] = repetition_penalty
        if response_format is not OMIT:
            _request["response_format"] = response_format
        if stop is not OMIT:
            _request["stop"] = stop
        if stream_options is not OMIT:
            _request["stream_options"] = stream_options
        if temperature is not OMIT:
            _request["temperature"] = temperature
        if top_logprobs is not OMIT:
            _request["top_logprobs"] = top_logprobs
        if top_p is not OMIT:
            _request["top_p"] = top_p
        if user is not OMIT:
            _request["user"] = user
        with self._client_wrapper.httpx_client.stream(
            method="POST",
            url=urllib.parse.urljoin(f"{self._client_wrapper.get_environment().text_gen}/", "v1/chat/completions"),
            params=jsonable_encoder(
                request_options.get("additional_query_parameters") if request_options is not None else None
            ),
            json=jsonable_encoder(_request)
            if request_options is None or request_options.get("additional_body_parameters") is None
            else {
                **jsonable_encoder(_request),
                **(jsonable_encoder(remove_none_from_dict(request_options.get("additional_body_parameters", {})))),
            },
            headers=jsonable_encoder(
                remove_none_from_dict(
                    {
                        **self._client_wrapper.get_headers(),
                        **(request_options.get("additional_headers", {}) if request_options is not None else {}),
                    }
                )
            ),
            timeout=request_options.get("timeout_in_seconds")
            if request_options is not None and request_options.get("timeout_in_seconds") is not None
            else self._client_wrapper.get_timeout(),
            retries=0,
            max_retries=request_options.get("max_retries") if request_options is not None else 0,  # type: ignore
        ) as _response:
            if 200 <= _response.status_code < 300:
                _event_source = httpx_sse.EventSource(_response)
                for _sse in _event_source.iter_sse():
                    yield pydantic_v1.parse_obj_as(ChatCompletionChunk, json.loads(_sse.data))  # type: ignore
                return
            _response.read()
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    pydantic_v1.parse_obj_as(HttpValidationError, _response.json())  # type: ignore
                )
            if _response.status_code == 500:
                raise InternalServerError(pydantic_v1.parse_obj_as(ErrorResponse, _response.json()))  # type: ignore
            try:
                _response_json = _response.json()
            except JSONDecodeError:
                raise ApiError(status_code=_response.status_code, body=_response.text)
            raise ApiError(status_code=_response.status_code, body=_response_json)

    def create_chat_completion(
        self,
        *,
        messages: typing.Sequence[ChatMessage],
        model: str,
        frequency_penalty: typing.Optional[float] = OMIT,
        ignore_eos: typing.Optional[bool] = OMIT,
        logit_bias: typing.Optional[typing.Dict[str, typing.Optional[float]]] = OMIT,
        loglikelihood: typing.Optional[bool] = OMIT,
        logprobs: typing.Optional[bool] = OMIT,
        max_tokens: typing.Optional[int] = OMIT,
        n: typing.Optional[int] = OMIT,
        octoai: typing.Optional[ChatCompletionRequestExt] = OMIT,
        presence_penalty: typing.Optional[float] = OMIT,
        repetition_penalty: typing.Optional[float] = OMIT,
        response_format: typing.Optional[ChatCompletionResponseFormat] = OMIT,
        stop: typing.Optional[Stop] = OMIT,
        stream_options: typing.Optional[StreamOptions] = OMIT,
        temperature: typing.Optional[float] = OMIT,
        top_logprobs: typing.Optional[int] = OMIT,
        top_p: typing.Optional[float] = OMIT,
        user: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> ChatCompletionResponse:
        """
        Create a Chat Completion.

        Parameters
        ----------
        messages : typing.Sequence[ChatMessage]
            A list of messages comprising the conversation so far.

        model : str
            The identifier of the model to use.Can be a shared tenancy or custom model identifier.

        frequency_penalty : typing.Optional[float]
            Penalizes new tokens based on their frequency in the generated text so far.

        ignore_eos : typing.Optional[bool]

        logit_bias : typing.Optional[typing.Dict[str, typing.Optional[float]]]

        loglikelihood : typing.Optional[bool]

        logprobs : typing.Optional[bool]

        max_tokens : typing.Optional[int]
            Maximum number of tokens to generate per output sequence.

        n : typing.Optional[int]

        octoai : typing.Optional[ChatCompletionRequestExt]

        presence_penalty : typing.Optional[float]
            Penalizes new tokens based on whether they appear in the generated text so far

        repetition_penalty : typing.Optional[float]
            Controls the likelihood of the model generating repeated texts

        response_format : typing.Optional[ChatCompletionResponseFormat]

        stop : typing.Optional[Stop]

        stream_options : typing.Optional[StreamOptions]

        temperature : typing.Optional[float]
            Controls the randomness of the sampling.

        top_logprobs : typing.Optional[int]

        top_p : typing.Optional[float]
            Controls the cumulative probability of the top tokens to consider.

        user : typing.Optional[str]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ChatCompletionResponse


        Examples
        --------
        from octoai.client import OctoAI
        from octoai.text_gen import ChatMessage

        client = OctoAI(
            api_key="YOUR_API_KEY",
        )
        client.text_gen.create_chat_completion(
            messages=[
                ChatMessage(
                    role="role",
                )
            ],
            model="model",
        )
        """
        _request: typing.Dict[str, typing.Any] = {"messages": messages, "model": model, "stream": False}
        if frequency_penalty is not OMIT:
            _request["frequency_penalty"] = frequency_penalty
        if ignore_eos is not OMIT:
            _request["ignore_eos"] = ignore_eos
        if logit_bias is not OMIT:
            _request["logit_bias"] = logit_bias
        if loglikelihood is not OMIT:
            _request["loglikelihood"] = loglikelihood
        if logprobs is not OMIT:
            _request["logprobs"] = logprobs
        if max_tokens is not OMIT:
            _request["max_tokens"] = max_tokens
        if n is not OMIT:
            _request["n"] = n
        if octoai is not OMIT:
            _request["octoai"] = octoai
        if presence_penalty is not OMIT:
            _request["presence_penalty"] = presence_penalty
        if repetition_penalty is not OMIT:
            _request["repetition_penalty"] = repetition_penalty
        if response_format is not OMIT:
            _request["response_format"] = response_format
        if stop is not OMIT:
            _request["stop"] = stop
        if stream_options is not OMIT:
            _request["stream_options"] = stream_options
        if temperature is not OMIT:
            _request["temperature"] = temperature
        if top_logprobs is not OMIT:
            _request["top_logprobs"] = top_logprobs
        if top_p is not OMIT:
            _request["top_p"] = top_p
        if user is not OMIT:
            _request["user"] = user
        _response = self._client_wrapper.httpx_client.request(
            method="POST",
            url=urllib.parse.urljoin(f"{self._client_wrapper.get_environment().text_gen}/", "v1/chat/completions"),
            params=jsonable_encoder(
                request_options.get("additional_query_parameters") if request_options is not None else None
            ),
            json=jsonable_encoder(_request)
            if request_options is None or request_options.get("additional_body_parameters") is None
            else {
                **jsonable_encoder(_request),
                **(jsonable_encoder(remove_none_from_dict(request_options.get("additional_body_parameters", {})))),
            },
            headers=jsonable_encoder(
                remove_none_from_dict(
                    {
                        **self._client_wrapper.get_headers(),
                        **(request_options.get("additional_headers", {}) if request_options is not None else {}),
                    }
                )
            ),
            timeout=request_options.get("timeout_in_seconds")
            if request_options is not None and request_options.get("timeout_in_seconds") is not None
            else self._client_wrapper.get_timeout(),
            retries=0,
            max_retries=request_options.get("max_retries") if request_options is not None else 0,  # type: ignore
        )
        if 200 <= _response.status_code < 300:
            return pydantic_v1.parse_obj_as(ChatCompletionResponse, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(
                pydantic_v1.parse_obj_as(HttpValidationError, _response.json())  # type: ignore
            )
        if _response.status_code == 500:
            raise InternalServerError(pydantic_v1.parse_obj_as(ErrorResponse, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def create_completion_stream(
        self,
        *,
        model: str,
        best_of: typing.Optional[int] = OMIT,
        echo: typing.Optional[bool] = OMIT,
        frequency_penalty: typing.Optional[float] = OMIT,
        logit_bias: typing.Optional[typing.Dict[str, typing.Optional[float]]] = OMIT,
        loglikelihood: typing.Optional[bool] = OMIT,
        logprobs: typing.Optional[int] = OMIT,
        max_tokens: typing.Optional[int] = OMIT,
        n: typing.Optional[int] = OMIT,
        presence_penalty: typing.Optional[float] = OMIT,
        prompt: typing.Optional[Prompt] = OMIT,
        repetition_penalty: typing.Optional[float] = OMIT,
        seed: typing.Optional[int] = OMIT,
        stop: typing.Optional[Stop] = OMIT,
        stream_options: typing.Optional[StreamOptions] = OMIT,
        suffix: typing.Optional[str] = OMIT,
        temperature: typing.Optional[float] = OMIT,
        top_p: typing.Optional[float] = OMIT,
        user: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.Iterator[CompletionResponse]:
        """
        Parameters
        ----------
        model : str
            Model name to use for completion.

        best_of : typing.Optional[int]
            Number of sequences that are generated from the prompt.`best_of` must be greater than or equal to `n`.

        echo : typing.Optional[bool]
            Echo back the prompt in addition to the completion.

        frequency_penalty : typing.Optional[float]
            Penalizes new tokens based on their frequency in the generated text so far.

        logit_bias : typing.Optional[typing.Dict[str, typing.Optional[float]]]

        loglikelihood : typing.Optional[bool]
            Switch on loglikelihood regime and return log probabilities from all prompt tokens from prefill.

        logprobs : typing.Optional[int]

        max_tokens : typing.Optional[int]
            Maximum number of tokens to generate per output sequence.

        n : typing.Optional[int]
            Number of output sequences to return.

        presence_penalty : typing.Optional[float]
            Penalizes new tokens based on whether they appear in the generated text so far

        prompt : typing.Optional[Prompt]
            The prompt to generate completions from.

        repetition_penalty : typing.Optional[float]
            Controls the likelihood of the model generating repeated texts

        seed : typing.Optional[int]

        stop : typing.Optional[Stop]
            Strings that stop the generation when they are generated.

        stream_options : typing.Optional[StreamOptions]

        suffix : typing.Optional[str]

        temperature : typing.Optional[float]
            Controls the randomness of the sampling.

        top_p : typing.Optional[float]
            Controls the cumulative probability of the top tokens to consider.

        user : typing.Optional[str]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Yields
        ------
        typing.Iterator[CompletionResponse]


        Examples
        --------
        from octoai.client import OctoAI
        from octoai.text_gen import StreamOptions

        client = OctoAI(
            api_key="YOUR_API_KEY",
        )
        client.text_gen.create_completion_stream(
            best_of=1,
            echo=True,
            frequency_penalty=1.1,
            logit_bias={"string": {"key": "value"}},
            loglikelihood=True,
            logprobs=1,
            max_tokens=1,
            model="string",
            n=1,
            presence_penalty=1.1,
            prompt="string",
            repetition_penalty=1.1,
            seed=1,
            stop="string",
            stream_options=StreamOptions(
                include_usage=True,
            ),
            suffix="string",
            temperature=1.1,
            top_p=1.1,
            user="string",
        )
        """
        _request: typing.Dict[str, typing.Any] = {"model": model, "stream": True}
        if best_of is not OMIT:
            _request["best_of"] = best_of
        if echo is not OMIT:
            _request["echo"] = echo
        if frequency_penalty is not OMIT:
            _request["frequency_penalty"] = frequency_penalty
        if logit_bias is not OMIT:
            _request["logit_bias"] = logit_bias
        if loglikelihood is not OMIT:
            _request["loglikelihood"] = loglikelihood
        if logprobs is not OMIT:
            _request["logprobs"] = logprobs
        if max_tokens is not OMIT:
            _request["max_tokens"] = max_tokens
        if n is not OMIT:
            _request["n"] = n
        if presence_penalty is not OMIT:
            _request["presence_penalty"] = presence_penalty
        if prompt is not OMIT:
            _request["prompt"] = prompt
        if repetition_penalty is not OMIT:
            _request["repetition_penalty"] = repetition_penalty
        if seed is not OMIT:
            _request["seed"] = seed
        if stop is not OMIT:
            _request["stop"] = stop
        if stream_options is not OMIT:
            _request["stream_options"] = stream_options
        if suffix is not OMIT:
            _request["suffix"] = suffix
        if temperature is not OMIT:
            _request["temperature"] = temperature
        if top_p is not OMIT:
            _request["top_p"] = top_p
        if user is not OMIT:
            _request["user"] = user
        with self._client_wrapper.httpx_client.stream(
            method="POST",
            url=urllib.parse.urljoin(f"{self._client_wrapper.get_environment().text_gen}/", "v1/completions"),
            params=jsonable_encoder(
                request_options.get("additional_query_parameters") if request_options is not None else None
            ),
            json=jsonable_encoder(_request)
            if request_options is None or request_options.get("additional_body_parameters") is None
            else {
                **jsonable_encoder(_request),
                **(jsonable_encoder(remove_none_from_dict(request_options.get("additional_body_parameters", {})))),
            },
            headers=jsonable_encoder(
                remove_none_from_dict(
                    {
                        **self._client_wrapper.get_headers(),
                        **(request_options.get("additional_headers", {}) if request_options is not None else {}),
                    }
                )
            ),
            timeout=request_options.get("timeout_in_seconds")
            if request_options is not None and request_options.get("timeout_in_seconds") is not None
            else self._client_wrapper.get_timeout(),
            retries=0,
            max_retries=request_options.get("max_retries") if request_options is not None else 0,  # type: ignore
        ) as _response:
            if 200 <= _response.status_code < 300:
                _event_source = httpx_sse.EventSource(_response)
                for _sse in _event_source.iter_sse():
                    yield pydantic_v1.parse_obj_as(CompletionResponse, json.loads(_sse.data))  # type: ignore
                return
            _response.read()
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    pydantic_v1.parse_obj_as(HttpValidationError, _response.json())  # type: ignore
                )
            if _response.status_code == 500:
                raise InternalServerError(pydantic_v1.parse_obj_as(ErrorResponse, _response.json()))  # type: ignore
            try:
                _response_json = _response.json()
            except JSONDecodeError:
                raise ApiError(status_code=_response.status_code, body=_response.text)
            raise ApiError(status_code=_response.status_code, body=_response_json)

    def create_completion(
        self,
        *,
        model: str,
        best_of: typing.Optional[int] = OMIT,
        echo: typing.Optional[bool] = OMIT,
        frequency_penalty: typing.Optional[float] = OMIT,
        logit_bias: typing.Optional[typing.Dict[str, typing.Optional[float]]] = OMIT,
        loglikelihood: typing.Optional[bool] = OMIT,
        logprobs: typing.Optional[int] = OMIT,
        max_tokens: typing.Optional[int] = OMIT,
        n: typing.Optional[int] = OMIT,
        presence_penalty: typing.Optional[float] = OMIT,
        prompt: typing.Optional[Prompt] = OMIT,
        repetition_penalty: typing.Optional[float] = OMIT,
        seed: typing.Optional[int] = OMIT,
        stop: typing.Optional[Stop] = OMIT,
        stream_options: typing.Optional[StreamOptions] = OMIT,
        suffix: typing.Optional[str] = OMIT,
        temperature: typing.Optional[float] = OMIT,
        top_p: typing.Optional[float] = OMIT,
        user: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> CompletionResponse:
        """
        Parameters
        ----------
        model : str
            Model name to use for completion.

        best_of : typing.Optional[int]
            Number of sequences that are generated from the prompt.`best_of` must be greater than or equal to `n`.

        echo : typing.Optional[bool]
            Echo back the prompt in addition to the completion.

        frequency_penalty : typing.Optional[float]
            Penalizes new tokens based on their frequency in the generated text so far.

        logit_bias : typing.Optional[typing.Dict[str, typing.Optional[float]]]

        loglikelihood : typing.Optional[bool]
            Switch on loglikelihood regime and return log probabilities from all prompt tokens from prefill.

        logprobs : typing.Optional[int]

        max_tokens : typing.Optional[int]
            Maximum number of tokens to generate per output sequence.

        n : typing.Optional[int]
            Number of output sequences to return.

        presence_penalty : typing.Optional[float]
            Penalizes new tokens based on whether they appear in the generated text so far

        prompt : typing.Optional[Prompt]
            The prompt to generate completions from.

        repetition_penalty : typing.Optional[float]
            Controls the likelihood of the model generating repeated texts

        seed : typing.Optional[int]

        stop : typing.Optional[Stop]
            Strings that stop the generation when they are generated.

        stream_options : typing.Optional[StreamOptions]

        suffix : typing.Optional[str]

        temperature : typing.Optional[float]
            Controls the randomness of the sampling.

        top_p : typing.Optional[float]
            Controls the cumulative probability of the top tokens to consider.

        user : typing.Optional[str]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        CompletionResponse


        Examples
        --------
        from octoai.client import OctoAI

        client = OctoAI(
            api_key="YOUR_API_KEY",
        )
        client.text_gen.create_completion(
            model="model",
        )
        """
        _request: typing.Dict[str, typing.Any] = {"model": model, "stream": False}
        if best_of is not OMIT:
            _request["best_of"] = best_of
        if echo is not OMIT:
            _request["echo"] = echo
        if frequency_penalty is not OMIT:
            _request["frequency_penalty"] = frequency_penalty
        if logit_bias is not OMIT:
            _request["logit_bias"] = logit_bias
        if loglikelihood is not OMIT:
            _request["loglikelihood"] = loglikelihood
        if logprobs is not OMIT:
            _request["logprobs"] = logprobs
        if max_tokens is not OMIT:
            _request["max_tokens"] = max_tokens
        if n is not OMIT:
            _request["n"] = n
        if presence_penalty is not OMIT:
            _request["presence_penalty"] = presence_penalty
        if prompt is not OMIT:
            _request["prompt"] = prompt
        if repetition_penalty is not OMIT:
            _request["repetition_penalty"] = repetition_penalty
        if seed is not OMIT:
            _request["seed"] = seed
        if stop is not OMIT:
            _request["stop"] = stop
        if stream_options is not OMIT:
            _request["stream_options"] = stream_options
        if suffix is not OMIT:
            _request["suffix"] = suffix
        if temperature is not OMIT:
            _request["temperature"] = temperature
        if top_p is not OMIT:
            _request["top_p"] = top_p
        if user is not OMIT:
            _request["user"] = user
        _response = self._client_wrapper.httpx_client.request(
            method="POST",
            url=urllib.parse.urljoin(f"{self._client_wrapper.get_environment().text_gen}/", "v1/completions"),
            params=jsonable_encoder(
                request_options.get("additional_query_parameters") if request_options is not None else None
            ),
            json=jsonable_encoder(_request)
            if request_options is None or request_options.get("additional_body_parameters") is None
            else {
                **jsonable_encoder(_request),
                **(jsonable_encoder(remove_none_from_dict(request_options.get("additional_body_parameters", {})))),
            },
            headers=jsonable_encoder(
                remove_none_from_dict(
                    {
                        **self._client_wrapper.get_headers(),
                        **(request_options.get("additional_headers", {}) if request_options is not None else {}),
                    }
                )
            ),
            timeout=request_options.get("timeout_in_seconds")
            if request_options is not None and request_options.get("timeout_in_seconds") is not None
            else self._client_wrapper.get_timeout(),
            retries=0,
            max_retries=request_options.get("max_retries") if request_options is not None else 0,  # type: ignore
        )
        if 200 <= _response.status_code < 300:
            return pydantic_v1.parse_obj_as(CompletionResponse, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(
                pydantic_v1.parse_obj_as(HttpValidationError, _response.json())  # type: ignore
            )
        if _response.status_code == 500:
            raise InternalServerError(pydantic_v1.parse_obj_as(ErrorResponse, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)


class AsyncTextGenClient:
    def __init__(self, *, client_wrapper: AsyncClientWrapper):
        self._client_wrapper = client_wrapper

    async def create_chat_completion_stream(
        self,
        *,
        messages: typing.Sequence[ChatMessage],
        model: str,
        frequency_penalty: typing.Optional[float] = OMIT,
        ignore_eos: typing.Optional[bool] = OMIT,
        logit_bias: typing.Optional[typing.Dict[str, typing.Optional[float]]] = OMIT,
        loglikelihood: typing.Optional[bool] = OMIT,
        logprobs: typing.Optional[bool] = OMIT,
        max_tokens: typing.Optional[int] = OMIT,
        n: typing.Optional[int] = OMIT,
        octoai: typing.Optional[ChatCompletionRequestExt] = OMIT,
        presence_penalty: typing.Optional[float] = OMIT,
        repetition_penalty: typing.Optional[float] = OMIT,
        response_format: typing.Optional[ChatCompletionResponseFormat] = OMIT,
        stop: typing.Optional[Stop] = OMIT,
        stream_options: typing.Optional[StreamOptions] = OMIT,
        temperature: typing.Optional[float] = OMIT,
        top_logprobs: typing.Optional[int] = OMIT,
        top_p: typing.Optional[float] = OMIT,
        user: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.AsyncIterator[ChatCompletionChunk]:
        """
        Create a Chat Completion.

        Parameters
        ----------
        messages : typing.Sequence[ChatMessage]
            A list of messages comprising the conversation so far.

        model : str
            The identifier of the model to use.Can be a shared tenancy or custom model identifier.

        frequency_penalty : typing.Optional[float]
            Penalizes new tokens based on their frequency in the generated text so far.

        ignore_eos : typing.Optional[bool]

        logit_bias : typing.Optional[typing.Dict[str, typing.Optional[float]]]

        loglikelihood : typing.Optional[bool]

        logprobs : typing.Optional[bool]

        max_tokens : typing.Optional[int]
            Maximum number of tokens to generate per output sequence.

        n : typing.Optional[int]

        octoai : typing.Optional[ChatCompletionRequestExt]

        presence_penalty : typing.Optional[float]
            Penalizes new tokens based on whether they appear in the generated text so far

        repetition_penalty : typing.Optional[float]
            Controls the likelihood of the model generating repeated texts

        response_format : typing.Optional[ChatCompletionResponseFormat]

        stop : typing.Optional[Stop]

        stream_options : typing.Optional[StreamOptions]

        temperature : typing.Optional[float]
            Controls the randomness of the sampling.

        top_logprobs : typing.Optional[int]

        top_p : typing.Optional[float]
            Controls the cumulative probability of the top tokens to consider.

        user : typing.Optional[str]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Yields
        ------
        typing.AsyncIterator[ChatCompletionChunk]


        Examples
        --------
        from octoai.client import AsyncOctoAI
        from octoai.text_gen import (
            ChatCompletionRequestExt,
            ChatCompletionRequestExtVllm,
            ChatCompletionResponseFormat,
            ChatMessage,
            StreamOptions,
        )

        client = AsyncOctoAI(
            api_key="YOUR_API_KEY",
        )
        await client.text_gen.create_chat_completion_stream(
            frequency_penalty=1.1,
            ignore_eos=True,
            logit_bias={"string": {"key": "value"}},
            loglikelihood=True,
            logprobs=True,
            max_tokens=1,
            messages=[
                ChatMessage(
                    content="string",
                    role="string",
                )
            ],
            model="string",
            n=1,
            octoai=ChatCompletionRequestExt(
                vllm=ChatCompletionRequestExtVllm(),
            ),
            presence_penalty=1.1,
            repetition_penalty=1.1,
            response_format=ChatCompletionResponseFormat(
                schema={"string": {"key": "value"}},
                type="string",
            ),
            stop="string",
            stream_options=StreamOptions(
                include_usage=True,
            ),
            temperature=1.1,
            top_logprobs=1,
            top_p=1.1,
            user="string",
        )
        """
        _request: typing.Dict[str, typing.Any] = {"messages": messages, "model": model, "stream": True}
        if frequency_penalty is not OMIT:
            _request["frequency_penalty"] = frequency_penalty
        if ignore_eos is not OMIT:
            _request["ignore_eos"] = ignore_eos
        if logit_bias is not OMIT:
            _request["logit_bias"] = logit_bias
        if loglikelihood is not OMIT:
            _request["loglikelihood"] = loglikelihood
        if logprobs is not OMIT:
            _request["logprobs"] = logprobs
        if max_tokens is not OMIT:
            _request["max_tokens"] = max_tokens
        if n is not OMIT:
            _request["n"] = n
        if octoai is not OMIT:
            _request["octoai"] = octoai
        if presence_penalty is not OMIT:
            _request["presence_penalty"] = presence_penalty
        if repetition_penalty is not OMIT:
            _request["repetition_penalty"] = repetition_penalty
        if response_format is not OMIT:
            _request["response_format"] = response_format
        if stop is not OMIT:
            _request["stop"] = stop
        if stream_options is not OMIT:
            _request["stream_options"] = stream_options
        if temperature is not OMIT:
            _request["temperature"] = temperature
        if top_logprobs is not OMIT:
            _request["top_logprobs"] = top_logprobs
        if top_p is not OMIT:
            _request["top_p"] = top_p
        if user is not OMIT:
            _request["user"] = user
        async with self._client_wrapper.httpx_client.stream(
            method="POST",
            url=urllib.parse.urljoin(f"{self._client_wrapper.get_environment().text_gen}/", "v1/chat/completions"),
            params=jsonable_encoder(
                request_options.get("additional_query_parameters") if request_options is not None else None
            ),
            json=jsonable_encoder(_request)
            if request_options is None or request_options.get("additional_body_parameters") is None
            else {
                **jsonable_encoder(_request),
                **(jsonable_encoder(remove_none_from_dict(request_options.get("additional_body_parameters", {})))),
            },
            headers=jsonable_encoder(
                remove_none_from_dict(
                    {
                        **self._client_wrapper.get_headers(),
                        **(request_options.get("additional_headers", {}) if request_options is not None else {}),
                    }
                )
            ),
            timeout=request_options.get("timeout_in_seconds")
            if request_options is not None and request_options.get("timeout_in_seconds") is not None
            else self._client_wrapper.get_timeout(),
            retries=0,
            max_retries=request_options.get("max_retries") if request_options is not None else 0,  # type: ignore
        ) as _response:
            if 200 <= _response.status_code < 300:
                _event_source = httpx_sse.EventSource(_response)
                async for _sse in _event_source.aiter_sse():
                    yield pydantic_v1.parse_obj_as(ChatCompletionChunk, json.loads(_sse.data))  # type: ignore
                return
            await _response.aread()
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    pydantic_v1.parse_obj_as(HttpValidationError, _response.json())  # type: ignore
                )
            if _response.status_code == 500:
                raise InternalServerError(pydantic_v1.parse_obj_as(ErrorResponse, _response.json()))  # type: ignore
            try:
                _response_json = _response.json()
            except JSONDecodeError:
                raise ApiError(status_code=_response.status_code, body=_response.text)
            raise ApiError(status_code=_response.status_code, body=_response_json)

    async def create_chat_completion(
        self,
        *,
        messages: typing.Sequence[ChatMessage],
        model: str,
        frequency_penalty: typing.Optional[float] = OMIT,
        ignore_eos: typing.Optional[bool] = OMIT,
        logit_bias: typing.Optional[typing.Dict[str, typing.Optional[float]]] = OMIT,
        loglikelihood: typing.Optional[bool] = OMIT,
        logprobs: typing.Optional[bool] = OMIT,
        max_tokens: typing.Optional[int] = OMIT,
        n: typing.Optional[int] = OMIT,
        octoai: typing.Optional[ChatCompletionRequestExt] = OMIT,
        presence_penalty: typing.Optional[float] = OMIT,
        repetition_penalty: typing.Optional[float] = OMIT,
        response_format: typing.Optional[ChatCompletionResponseFormat] = OMIT,
        stop: typing.Optional[Stop] = OMIT,
        stream_options: typing.Optional[StreamOptions] = OMIT,
        temperature: typing.Optional[float] = OMIT,
        top_logprobs: typing.Optional[int] = OMIT,
        top_p: typing.Optional[float] = OMIT,
        user: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> ChatCompletionResponse:
        """
        Create a Chat Completion.

        Parameters
        ----------
        messages : typing.Sequence[ChatMessage]
            A list of messages comprising the conversation so far.

        model : str
            The identifier of the model to use.Can be a shared tenancy or custom model identifier.

        frequency_penalty : typing.Optional[float]
            Penalizes new tokens based on their frequency in the generated text so far.

        ignore_eos : typing.Optional[bool]

        logit_bias : typing.Optional[typing.Dict[str, typing.Optional[float]]]

        loglikelihood : typing.Optional[bool]

        logprobs : typing.Optional[bool]

        max_tokens : typing.Optional[int]
            Maximum number of tokens to generate per output sequence.

        n : typing.Optional[int]

        octoai : typing.Optional[ChatCompletionRequestExt]

        presence_penalty : typing.Optional[float]
            Penalizes new tokens based on whether they appear in the generated text so far

        repetition_penalty : typing.Optional[float]
            Controls the likelihood of the model generating repeated texts

        response_format : typing.Optional[ChatCompletionResponseFormat]

        stop : typing.Optional[Stop]

        stream_options : typing.Optional[StreamOptions]

        temperature : typing.Optional[float]
            Controls the randomness of the sampling.

        top_logprobs : typing.Optional[int]

        top_p : typing.Optional[float]
            Controls the cumulative probability of the top tokens to consider.

        user : typing.Optional[str]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ChatCompletionResponse


        Examples
        --------
        from octoai.client import AsyncOctoAI
        from octoai.text_gen import ChatMessage

        client = AsyncOctoAI(
            api_key="YOUR_API_KEY",
        )
        await client.text_gen.create_chat_completion(
            messages=[
                ChatMessage(
                    role="role",
                )
            ],
            model="model",
        )
        """
        _request: typing.Dict[str, typing.Any] = {"messages": messages, "model": model, "stream": False}
        if frequency_penalty is not OMIT:
            _request["frequency_penalty"] = frequency_penalty
        if ignore_eos is not OMIT:
            _request["ignore_eos"] = ignore_eos
        if logit_bias is not OMIT:
            _request["logit_bias"] = logit_bias
        if loglikelihood is not OMIT:
            _request["loglikelihood"] = loglikelihood
        if logprobs is not OMIT:
            _request["logprobs"] = logprobs
        if max_tokens is not OMIT:
            _request["max_tokens"] = max_tokens
        if n is not OMIT:
            _request["n"] = n
        if octoai is not OMIT:
            _request["octoai"] = octoai
        if presence_penalty is not OMIT:
            _request["presence_penalty"] = presence_penalty
        if repetition_penalty is not OMIT:
            _request["repetition_penalty"] = repetition_penalty
        if response_format is not OMIT:
            _request["response_format"] = response_format
        if stop is not OMIT:
            _request["stop"] = stop
        if stream_options is not OMIT:
            _request["stream_options"] = stream_options
        if temperature is not OMIT:
            _request["temperature"] = temperature
        if top_logprobs is not OMIT:
            _request["top_logprobs"] = top_logprobs
        if top_p is not OMIT:
            _request["top_p"] = top_p
        if user is not OMIT:
            _request["user"] = user
        _response = await self._client_wrapper.httpx_client.request(
            method="POST",
            url=urllib.parse.urljoin(f"{self._client_wrapper.get_environment().text_gen}/", "v1/chat/completions"),
            params=jsonable_encoder(
                request_options.get("additional_query_parameters") if request_options is not None else None
            ),
            json=jsonable_encoder(_request)
            if request_options is None or request_options.get("additional_body_parameters") is None
            else {
                **jsonable_encoder(_request),
                **(jsonable_encoder(remove_none_from_dict(request_options.get("additional_body_parameters", {})))),
            },
            headers=jsonable_encoder(
                remove_none_from_dict(
                    {
                        **self._client_wrapper.get_headers(),
                        **(request_options.get("additional_headers", {}) if request_options is not None else {}),
                    }
                )
            ),
            timeout=request_options.get("timeout_in_seconds")
            if request_options is not None and request_options.get("timeout_in_seconds") is not None
            else self._client_wrapper.get_timeout(),
            retries=0,
            max_retries=request_options.get("max_retries") if request_options is not None else 0,  # type: ignore
        )
        if 200 <= _response.status_code < 300:
            return pydantic_v1.parse_obj_as(ChatCompletionResponse, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(
                pydantic_v1.parse_obj_as(HttpValidationError, _response.json())  # type: ignore
            )
        if _response.status_code == 500:
            raise InternalServerError(pydantic_v1.parse_obj_as(ErrorResponse, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def create_completion_stream(
        self,
        *,
        model: str,
        best_of: typing.Optional[int] = OMIT,
        echo: typing.Optional[bool] = OMIT,
        frequency_penalty: typing.Optional[float] = OMIT,
        logit_bias: typing.Optional[typing.Dict[str, typing.Optional[float]]] = OMIT,
        loglikelihood: typing.Optional[bool] = OMIT,
        logprobs: typing.Optional[int] = OMIT,
        max_tokens: typing.Optional[int] = OMIT,
        n: typing.Optional[int] = OMIT,
        presence_penalty: typing.Optional[float] = OMIT,
        prompt: typing.Optional[Prompt] = OMIT,
        repetition_penalty: typing.Optional[float] = OMIT,
        seed: typing.Optional[int] = OMIT,
        stop: typing.Optional[Stop] = OMIT,
        stream_options: typing.Optional[StreamOptions] = OMIT,
        suffix: typing.Optional[str] = OMIT,
        temperature: typing.Optional[float] = OMIT,
        top_p: typing.Optional[float] = OMIT,
        user: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.AsyncIterator[CompletionResponse]:
        """
        Parameters
        ----------
        model : str
            Model name to use for completion.

        best_of : typing.Optional[int]
            Number of sequences that are generated from the prompt.`best_of` must be greater than or equal to `n`.

        echo : typing.Optional[bool]
            Echo back the prompt in addition to the completion.

        frequency_penalty : typing.Optional[float]
            Penalizes new tokens based on their frequency in the generated text so far.

        logit_bias : typing.Optional[typing.Dict[str, typing.Optional[float]]]

        loglikelihood : typing.Optional[bool]
            Switch on loglikelihood regime and return log probabilities from all prompt tokens from prefill.

        logprobs : typing.Optional[int]

        max_tokens : typing.Optional[int]
            Maximum number of tokens to generate per output sequence.

        n : typing.Optional[int]
            Number of output sequences to return.

        presence_penalty : typing.Optional[float]
            Penalizes new tokens based on whether they appear in the generated text so far

        prompt : typing.Optional[Prompt]
            The prompt to generate completions from.

        repetition_penalty : typing.Optional[float]
            Controls the likelihood of the model generating repeated texts

        seed : typing.Optional[int]

        stop : typing.Optional[Stop]
            Strings that stop the generation when they are generated.

        stream_options : typing.Optional[StreamOptions]

        suffix : typing.Optional[str]

        temperature : typing.Optional[float]
            Controls the randomness of the sampling.

        top_p : typing.Optional[float]
            Controls the cumulative probability of the top tokens to consider.

        user : typing.Optional[str]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Yields
        ------
        typing.AsyncIterator[CompletionResponse]


        Examples
        --------
        from octoai.client import AsyncOctoAI
        from octoai.text_gen import StreamOptions

        client = AsyncOctoAI(
            api_key="YOUR_API_KEY",
        )
        await client.text_gen.create_completion_stream(
            best_of=1,
            echo=True,
            frequency_penalty=1.1,
            logit_bias={"string": {"key": "value"}},
            loglikelihood=True,
            logprobs=1,
            max_tokens=1,
            model="string",
            n=1,
            presence_penalty=1.1,
            prompt="string",
            repetition_penalty=1.1,
            seed=1,
            stop="string",
            stream_options=StreamOptions(
                include_usage=True,
            ),
            suffix="string",
            temperature=1.1,
            top_p=1.1,
            user="string",
        )
        """
        _request: typing.Dict[str, typing.Any] = {"model": model, "stream": True}
        if best_of is not OMIT:
            _request["best_of"] = best_of
        if echo is not OMIT:
            _request["echo"] = echo
        if frequency_penalty is not OMIT:
            _request["frequency_penalty"] = frequency_penalty
        if logit_bias is not OMIT:
            _request["logit_bias"] = logit_bias
        if loglikelihood is not OMIT:
            _request["loglikelihood"] = loglikelihood
        if logprobs is not OMIT:
            _request["logprobs"] = logprobs
        if max_tokens is not OMIT:
            _request["max_tokens"] = max_tokens
        if n is not OMIT:
            _request["n"] = n
        if presence_penalty is not OMIT:
            _request["presence_penalty"] = presence_penalty
        if prompt is not OMIT:
            _request["prompt"] = prompt
        if repetition_penalty is not OMIT:
            _request["repetition_penalty"] = repetition_penalty
        if seed is not OMIT:
            _request["seed"] = seed
        if stop is not OMIT:
            _request["stop"] = stop
        if stream_options is not OMIT:
            _request["stream_options"] = stream_options
        if suffix is not OMIT:
            _request["suffix"] = suffix
        if temperature is not OMIT:
            _request["temperature"] = temperature
        if top_p is not OMIT:
            _request["top_p"] = top_p
        if user is not OMIT:
            _request["user"] = user
        async with self._client_wrapper.httpx_client.stream(
            method="POST",
            url=urllib.parse.urljoin(f"{self._client_wrapper.get_environment().text_gen}/", "v1/completions"),
            params=jsonable_encoder(
                request_options.get("additional_query_parameters") if request_options is not None else None
            ),
            json=jsonable_encoder(_request)
            if request_options is None or request_options.get("additional_body_parameters") is None
            else {
                **jsonable_encoder(_request),
                **(jsonable_encoder(remove_none_from_dict(request_options.get("additional_body_parameters", {})))),
            },
            headers=jsonable_encoder(
                remove_none_from_dict(
                    {
                        **self._client_wrapper.get_headers(),
                        **(request_options.get("additional_headers", {}) if request_options is not None else {}),
                    }
                )
            ),
            timeout=request_options.get("timeout_in_seconds")
            if request_options is not None and request_options.get("timeout_in_seconds") is not None
            else self._client_wrapper.get_timeout(),
            retries=0,
            max_retries=request_options.get("max_retries") if request_options is not None else 0,  # type: ignore
        ) as _response:
            if 200 <= _response.status_code < 300:
                _event_source = httpx_sse.EventSource(_response)
                async for _sse in _event_source.aiter_sse():
                    yield pydantic_v1.parse_obj_as(CompletionResponse, json.loads(_sse.data))  # type: ignore
                return
            await _response.aread()
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    pydantic_v1.parse_obj_as(HttpValidationError, _response.json())  # type: ignore
                )
            if _response.status_code == 500:
                raise InternalServerError(pydantic_v1.parse_obj_as(ErrorResponse, _response.json()))  # type: ignore
            try:
                _response_json = _response.json()
            except JSONDecodeError:
                raise ApiError(status_code=_response.status_code, body=_response.text)
            raise ApiError(status_code=_response.status_code, body=_response_json)

    async def create_completion(
        self,
        *,
        model: str,
        best_of: typing.Optional[int] = OMIT,
        echo: typing.Optional[bool] = OMIT,
        frequency_penalty: typing.Optional[float] = OMIT,
        logit_bias: typing.Optional[typing.Dict[str, typing.Optional[float]]] = OMIT,
        loglikelihood: typing.Optional[bool] = OMIT,
        logprobs: typing.Optional[int] = OMIT,
        max_tokens: typing.Optional[int] = OMIT,
        n: typing.Optional[int] = OMIT,
        presence_penalty: typing.Optional[float] = OMIT,
        prompt: typing.Optional[Prompt] = OMIT,
        repetition_penalty: typing.Optional[float] = OMIT,
        seed: typing.Optional[int] = OMIT,
        stop: typing.Optional[Stop] = OMIT,
        stream_options: typing.Optional[StreamOptions] = OMIT,
        suffix: typing.Optional[str] = OMIT,
        temperature: typing.Optional[float] = OMIT,
        top_p: typing.Optional[float] = OMIT,
        user: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> CompletionResponse:
        """
        Parameters
        ----------
        model : str
            Model name to use for completion.

        best_of : typing.Optional[int]
            Number of sequences that are generated from the prompt.`best_of` must be greater than or equal to `n`.

        echo : typing.Optional[bool]
            Echo back the prompt in addition to the completion.

        frequency_penalty : typing.Optional[float]
            Penalizes new tokens based on their frequency in the generated text so far.

        logit_bias : typing.Optional[typing.Dict[str, typing.Optional[float]]]

        loglikelihood : typing.Optional[bool]
            Switch on loglikelihood regime and return log probabilities from all prompt tokens from prefill.

        logprobs : typing.Optional[int]

        max_tokens : typing.Optional[int]
            Maximum number of tokens to generate per output sequence.

        n : typing.Optional[int]
            Number of output sequences to return.

        presence_penalty : typing.Optional[float]
            Penalizes new tokens based on whether they appear in the generated text so far

        prompt : typing.Optional[Prompt]
            The prompt to generate completions from.

        repetition_penalty : typing.Optional[float]
            Controls the likelihood of the model generating repeated texts

        seed : typing.Optional[int]

        stop : typing.Optional[Stop]
            Strings that stop the generation when they are generated.

        stream_options : typing.Optional[StreamOptions]

        suffix : typing.Optional[str]

        temperature : typing.Optional[float]
            Controls the randomness of the sampling.

        top_p : typing.Optional[float]
            Controls the cumulative probability of the top tokens to consider.

        user : typing.Optional[str]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        CompletionResponse


        Examples
        --------
        from octoai.client import AsyncOctoAI

        client = AsyncOctoAI(
            api_key="YOUR_API_KEY",
        )
        await client.text_gen.create_completion(
            model="model",
        )
        """
        _request: typing.Dict[str, typing.Any] = {"model": model, "stream": False}
        if best_of is not OMIT:
            _request["best_of"] = best_of
        if echo is not OMIT:
            _request["echo"] = echo
        if frequency_penalty is not OMIT:
            _request["frequency_penalty"] = frequency_penalty
        if logit_bias is not OMIT:
            _request["logit_bias"] = logit_bias
        if loglikelihood is not OMIT:
            _request["loglikelihood"] = loglikelihood
        if logprobs is not OMIT:
            _request["logprobs"] = logprobs
        if max_tokens is not OMIT:
            _request["max_tokens"] = max_tokens
        if n is not OMIT:
            _request["n"] = n
        if presence_penalty is not OMIT:
            _request["presence_penalty"] = presence_penalty
        if prompt is not OMIT:
            _request["prompt"] = prompt
        if repetition_penalty is not OMIT:
            _request["repetition_penalty"] = repetition_penalty
        if seed is not OMIT:
            _request["seed"] = seed
        if stop is not OMIT:
            _request["stop"] = stop
        if stream_options is not OMIT:
            _request["stream_options"] = stream_options
        if suffix is not OMIT:
            _request["suffix"] = suffix
        if temperature is not OMIT:
            _request["temperature"] = temperature
        if top_p is not OMIT:
            _request["top_p"] = top_p
        if user is not OMIT:
            _request["user"] = user
        _response = await self._client_wrapper.httpx_client.request(
            method="POST",
            url=urllib.parse.urljoin(f"{self._client_wrapper.get_environment().text_gen}/", "v1/completions"),
            params=jsonable_encoder(
                request_options.get("additional_query_parameters") if request_options is not None else None
            ),
            json=jsonable_encoder(_request)
            if request_options is None or request_options.get("additional_body_parameters") is None
            else {
                **jsonable_encoder(_request),
                **(jsonable_encoder(remove_none_from_dict(request_options.get("additional_body_parameters", {})))),
            },
            headers=jsonable_encoder(
                remove_none_from_dict(
                    {
                        **self._client_wrapper.get_headers(),
                        **(request_options.get("additional_headers", {}) if request_options is not None else {}),
                    }
                )
            ),
            timeout=request_options.get("timeout_in_seconds")
            if request_options is not None and request_options.get("timeout_in_seconds") is not None
            else self._client_wrapper.get_timeout(),
            retries=0,
            max_retries=request_options.get("max_retries") if request_options is not None else 0,  # type: ignore
        )
        if 200 <= _response.status_code < 300:
            return pydantic_v1.parse_obj_as(CompletionResponse, _response.json())  # type: ignore
        if _response.status_code == 422:
            raise UnprocessableEntityError(
                pydantic_v1.parse_obj_as(HttpValidationError, _response.json())  # type: ignore
            )
        if _response.status_code == 500:
            raise InternalServerError(pydantic_v1.parse_obj_as(ErrorResponse, _response.json()))  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)
