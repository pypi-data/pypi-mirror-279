{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization in Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('../src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "\n",
    "from optymus import Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # Initialize weights and biases\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Forward pass\n",
    "        self.Z1 = jnp.dot(X, self.W1) + self.b1\n",
    "        self.A1 = jnp.tanh(self.Z1)\n",
    "        self.Z2 = jnp.dot(self.A1, self.W2) + self.b2\n",
    "        return self.Z2\n",
    "\n",
    "    def loss(self, X, y):\n",
    "        # Mean Squared Error Loss\n",
    "        y_pred = self.forward(X)\n",
    "        return jnp.mean((y_pred - y) ** 2)\n",
    "\n",
    "    def get_params(self):\n",
    "        # Get network parameters\n",
    "        return [self.W1, self.b1, self.W2, self.b2]\n",
    "\n",
    "    def set_params(self, params):\n",
    "        # Set network parameters\n",
    "        self.W1, self.b1, self.W2, self.b2 = params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_neural_network(X_train, y_train, nn, learning_rate=0.01, max_iter=100, tol=1e-4, verbose=True, optim='gradient_descent'):\n",
    "    # Flatten the parameters to optimize\n",
    "    params = nn.get_params()\n",
    "    flat_params = np.concatenate([p.flatten() for p in params])\n",
    "\n",
    "    def f_obj(flat_params):\n",
    "        # Unflatten the parameters\n",
    "        shapes = [p.shape for p in params]\n",
    "        sizes = [np.prod(shape) for shape in shapes]\n",
    "        new_params = []\n",
    "        index = 0\n",
    "        for size, shape in zip(sizes, shapes):\n",
    "            new_params.append(flat_params[index:index + size].reshape(shape))\n",
    "            index += size\n",
    "        nn.set_params(new_params)\n",
    "        return nn.loss(X_train, y_train)\n",
    "\n",
    "    opt = Optimizer(f_obj=f_obj, x0=flat_params, learning_rate=learning_rate, max_iter=max_iter, tol=tol, verbose=verbose, method=optim)\n",
    "    result = opt.get_results()\n",
    "\n",
    "    # Set the optimal parameters back to the network\n",
    "    shapes = [p.shape for p in params]\n",
    "    sizes = [np.prod(shape) for shape in shapes]\n",
    "    params = []\n",
    "    index = 0\n",
    "    for size, shape in zip(sizes, shapes):\n",
    "        params.append(result['xopt'][index:index + size].reshape(shape))\n",
    "        index += size\n",
    "    nn.set_params(params)\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gradient Descent 0:   1%|          | 1/100 [00:02<04:42,  2.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal parameters: [-1.86560232e+00 -1.66547550e+00 -1.67026722e+00 -1.24296492e+00\n",
      " -4.37178516e-01 -7.85577189e-01 -6.89505199e-01 -6.75997213e-01\n",
      " -3.75554626e-01 -3.64309780e-01  3.02878074e-01  8.17808429e+00\n",
      "  3.39393760e-01  2.52981315e+00  8.18217049e-01 -1.71098583e+01\n",
      " -3.24934367e+01 -1.55676768e+01 -1.95158962e+01 -1.60956517e+01\n",
      "  2.46781829e-01  3.70799709e-03  1.42065618e-03  1.51999486e-02\n",
      "  1.71958931e-02  1.63756928e-15]\n",
      "Minimum loss: 8.34156836344146\n",
      "Number of iterations: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate some sample data\n",
    "np.random.seed(0)\n",
    "X_train = np.random.randn(100, 3)\n",
    "y_train = np.dot(X_train, np.array([1.5, -2.0, 1.0])) + 0.5 * np.random.randn(100)\n",
    "\n",
    "# Define the neural network\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = 5\n",
    "output_size = 1\n",
    "nn = SimpleNeuralNetwork(input_size, hidden_size, output_size)\n",
    "\n",
    "# Train the neural network\n",
    "result = train_neural_network(X_train, y_train, nn, learning_rate=0.01, max_iter=100, tol=1e-4, verbose=True, optim='powell')\n",
    "\n",
    "# Print the results\n",
    "print(\"Optimal parameters:\", result['xopt'])\n",
    "print(\"Minimum loss:\", result['fmin'])\n",
    "print(\"Number of iterations:\", result['num_iter'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optymus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
