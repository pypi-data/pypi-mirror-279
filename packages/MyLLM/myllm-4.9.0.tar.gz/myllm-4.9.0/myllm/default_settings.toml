########################################
###          DEFAULT SETTINGS        ###
########################################

# Any of those settings can be changed
# by the user. To overwrite a setting, 
# create a settings.toml or load the 
# settings from .env file or vars.
# As an example, to disable the 
# myllm object:
# settings.toml
[default]
# Dynaconf settings verification
# use for testing
VALUE = "On default" 

# Module Enable/Disable
myllm_enabled = true


[default.myllm.template]
library = "g4f" # options are openai or g4f
enabled = false # options are true or false to enable or disable the llm
llm_model= "" # model to use e.g. gpt-3.5-turbo, gpt-4, gpt-4-32k
llm_provider = "g4f.Provider.Llama2" # only for g4f. Refer to https://github.com/xtekky/gpt4free
llm_provider_key = "" # only for bard and openai to pass either the cookie or the api key
max_memory = 100 # Conversation history size
load_history = false # load conversation history via a json file
timeout = 5 # time lag to wait ai response
llm_prefix = "üêª" # prefix use to filter the AI response
llm_template = """
You are a friendly AI, helping me with general tasks. 
Be courteuous, simple and direct.
"""

# [default.myllm.localai] 
# library = "openai"
# enabled = true
# llm_model= "" 
# llm_provider = "" 
# llm_provider_key = "DEADBE4F"
# llm_base_url = "http://localhost:8080"
# max_memory = 100
# load_history = false
# timeout = 5 
# llm_prefix = ""
# llm_template = """
# You are a friendly AI, helping me with 
# general tasks. Be courteuous, simple and direct.
# """

# [default.myllm.gpt35] 
# library = "openai"
# enabled = true
# llm_model= "gpt-3.5-turbo-0125" 
# llm_provider = "" 
# llm_provider_key = "DEADBE4F"
# max_memory = 100 
# timeout = 5
# load_history = false
# llm_prefix = ""
# llm_template = """
# You are a friendly AI, helping me with general tasks. 
# Be courteuous, simple and direct.
# """

# [default.myllm.llama2]
# enabled = true
# library = "g4f"
# llm_model= ""
# llm_provider = "g4f.Provider.Llama2"
# llm_provider_key = ""
# max_memory = 10
# load_history = false
# timeout = 1 
# llm_prefix = "" 
# llm_template = """
# You are a friendly AI, helping me with general tasks. 
# Be courteuous, simple and direct.
# """

# [default.myllm.bing]
# enabled = true
# library = "g4f"
# llm_model = ""
# llm_provider = "g4f.Provider.Bing"
# llm_provider_key = ""
# max_memory = 10 
# load_history = false
# timeout = 2 
# llm_prefix = "" 
# llm_template = """
# You are a friendly AI, helping me with general tasks. 
# Be courteuous, simple and direct.
# """

# [default.myllm.petals]
# library = "petals"
# enabled = true
# llm_model= ""
# llm_provider = "" 
# llm_provider_key = ""
# max_memory = 5 # Conversation history size
# timeout = 10 # time lag to wait ai response
# llm_prefix = "" # prefix use to filter the AI response
# llm_template = """
# You are a friendly AI, helping me with general tasks. 
# Be courteuous, simple and direct.
# """

########################################
###     END OF DEFAULT SETTINGS      ###
########################################