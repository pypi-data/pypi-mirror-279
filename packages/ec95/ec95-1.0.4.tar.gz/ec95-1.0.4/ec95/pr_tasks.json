{
  "1. Аномалии, трендь, сглаживание": {
    "Аномалии": {
      "распределение Стьюдента": "\n# распределение Стьюдента\n\ndata = pd.read_excel('df_example (1).xlsx')\n\n# визуальная оценка\nplt.plot(data.Close);\nsns.boxplot(data.Close);\n\ndata['tau'] = abs(data.Close - data.Close.mean()) / data.Close.std()\n\nn = len(data)\n\n# критические значения\ntau_cr_005 = (st.t(n - 2).ppf(1 - 0.05 / 2) * np.sqrt(n - 1)) / (np.sqrt(n - 2 + st.t(n - 2).ppf(1 - 0.05 / 2) ** 2))\n\ntau_cr_0001 = (st.t(n - 2).ppf(1 - 0.001 / 2) * np.sqrt(n - 1)) / (np.sqrt(n - 2 + st.t(n - 2).ppf(1 - 0.001 / 2) ** 2))\n\n# t < tau_cr_005 не аномалия\n# tau_cr_005 < t < tau_cr_0001 может быть аномалией\n# tau_cr_0001 < t аномалия",
      "метод Ирвина": "# пример для колонки clode\ndata['lambd'] = abs(data.Close - data.Close.shift(1)) / data.Close.std()\n\n# n   | P=0.95 | P=0.99\n# 2   |    2.8 |    3.7 \n# 3   |    2.2 |    2.9 \n# 10  |    1.5 |    2.0 \n# 20  |    1.3 |    1.8 \n# 30  |    1.2 |    1.7 \n# 50  |    1.1 |    1.6 \n# 100 |    1.0 |    1.5\n\n# исходя из n и alpha выбираем подходящее критическое значение\nn = 49\nalpha = 0.05\n# =>\ncr = 1.1\n\ndata[data['lambd'] >= 1.1]"
    },
    "Тренд": {
      "разность средних уровней": "\n# разность средних уровней\n\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as scs\n\ndata = pd.read_excel('your_path.xlsx')\n\ndef diff_average_levels(data, alpha=0.05):\n    delim = len(data)//2\n    first = data.iloc[:delim]\n    second = data.iloc[delim:]\n\n    n1 = len(first)\n    n2 = len(second)\n\n    f_mean = sum(first)/n1\n    s_mean = sum(second)/n2\n\n    f_var = sum((first - f_mean)**2) / (n1 - 1)\n    s_var = sum((second - s_mean)**2) / (n2 - 1)\n\n    F = max(f_var, s_var) / min(f_var, s_var)\n    f_stat = scs.f(n1 - 1, n2 - 1).isf(alpha)\n    assert F < f_stat, 'дисперсии не равны, метод не сможет дать ответ'\n\n    sigma = np.sqrt(((n1 - 1) * f_var + (n2 - 1) * s_var)/(n1 + n2 - 2))\n    t_stat = abs(f_mean - s_mean)/(sigma * np.sqrt(1/n1 + 1/n2))\n    t_table = scs.t(n1 + n2 -2).isf(alpha)\n\n    if t_stat < t_table:\n        print('Тренда нет')\n    else:\n        print('Тренд есть')\n\ndiff_average_levels(data['Close'])",
      "критерий серий": "\n# критерий серий\n\ntemp = data.copy()\n\ntemp['delta'] = np.where(temp.y > temp.y.median(), 1, 0)\n\nmx_len = 1\ncurrent = temp.delta[0]\ncurrent_len = 1\nseria_count = 1\n\nfor d in temp.delta.iloc[1:]:\n    if d == current:\n        current_len += 1\n    else:\n        current_len = 1\n        current = d\n        seria_count += 1\n    if current_len > mx_len:\n        mx_len = current_len\n\nprint(mx_len, seria_count) #108, 62\n\n# Если хотя бы одно из неравенств нарушается, то гипотеза об отсутствии тренда отвергается.\n\n# $\n# \\begin{cases}\n# \\tau_{max}(n) < [3,3(ln(n)+1)]> \\\\\n# \\nu > [\\frac1 2 (n+1-1,96\\sqrt{n-1})]\n# \\end{cases}\n# $\n\n108 < round(3.3 * (np.log(397) + 1)), 62 > round(0.5 * (397 + 1 - 1.96 * np.sqrt(397 - 1)))\n",
      "критерий Фостера-Стьюрта": "\ndef Foster(y, x):\n    n = len(y)\n\n    k = []\n    l = []\n    for i in range(len(y)):\n        if max(y[:(i + 1)]) == y[i]:\n            k.append(1)\n        else:\n            k.append(0)\n\n        if min(y[:(i + 1)]) == y[i]:\n            l.append(1)\n        else:\n            l.append(0)\n\n    s = sum(k) + sum(l)\n    d = sum(k) - sum(l)\n\n    mu = (1.693872 * np.log(n) - 0.299015) / (1 - 0.035092 * np.log(n) + 0.002705 * np.log(n) ** 2)\n    sigma1 = np.sqrt(2 * np.log(n) - 3.4253)\n    sigma2 = np.sqrt(2 * np.log(n) - 0.8456)\n\n    ts = abs(s - mu) / sigma1\n    td = abs(d - 0) / sigma2\n\n    t_crit = scs.t(n).isf(0.05)\n    \n    is_trend = td < t_crit\n\n    is_var = ts < t_crit\n\n    return f\"{ts=}\\n{td=}\\n{t_crit=}\\n{is_trend=}\\n{is_var=}\\n\"\n\nprint(Foster(data.iloc[:, 1], data.index))\n\n# ts=4.526167684396396\n# td=4.974669057169713\n# t_crit=1.7011309342659315\n# is_trend=False\n# is_var=False\n"
    },
    "Сглаживание": "\nimport pandas as pd\nimport numpy as np\n\n# Создание фиктивного DataFrame с временными рядами\nnp.random.seed(42)  # для воспроизводимости\ndata = {\n    'date': pd.date_range(start='2020-01-01', periods=100, freq='M'),\n    'value': np.random.normal(loc=100, scale=10, size=100)  # фиктивные данные\n}\ndf = pd.DataFrame(data)\n\n# Простая (среднеарифметическая) скользящая средняя\ndef simple_moving_average(data, p):\n    return data.rolling(window=2*p+1, center=True).mean()\n\n# Взвешенная (средневзвешенная) скользящая средняя\ndef weighted_moving_average(data, weights):\n    p = len(weights) // 2\n    def weighted_avg(x):\n        return np.sum(weights * x) / np.sum(weights)\n    return data.rolling(window=len(weights), center=True).apply(weighted_avg, raw=True)\n\n# Среднехронологическая\ndef chronological_average(data, T):\n    result = []\n    half_T = T // 2\n    for t in range(len(data)):\n        if t < half_T or t >= len(data) - half_T:\n            result.append(np.nan)\n        else:\n            avg = (0.5 * data[t - half_T] + np.sum(data[t - half_T + 1 : t + half_T]) + 0.5 * data[t + half_T]) / T\n            result.append(avg)\n    return pd.Series(result, index=data.index)\n\n# Важно отметить, что методы выше оставляют крайние точки несглаженными. Эту проблему можно решить двумя способами:\n# - Исключить крайние точки из ряда\n# - Применить специальные формулы сглаживания для крайних точек, например для трёх точек:\n#     - $\\tilde y_1 = \\frac{5 y_1 + 2 y_2 - y_3} 6$\n#     - $\\tilde y_n = \\frac{5 y_n + 2 y_{n-1} - y_{n-2}} 6$\n\n# Экспоненциальное сглаживание\ndef exponential_smoothing(data, alpha):\n    result = [data[0]]\n    for n in range(1, len(data)):\n        result.append(alpha * data[n] + (1 - alpha) * result[n-1])\n    return pd.Series(result, index=data.index)\n"
  },
  "2. Тренд сезонные модели": {
    "Кривая роста": "\n# кривая роста\n\ndf_curve1 = #read data\n\n# сглаживание ряда скользящей средней\n\n# корреляция до\nplt.figure(figsize=(10, 8))\nplt.title('Корреляционная матрица')\nsns.heatmap(df_curve1[['t', 'rolling_mean']].corr(), annot=True, vmax=1, vmin=-1);\n\ndf_curve1['rolling_mean'] = df_curve1['y'].rolling(window=4).mean()\n\ndf_curve1.dropna(inplace=True)\n\n# корреляция после, должна стать выше\nplt.figure(figsize=(10, 8))\nplt.title('Корреляционная матрица')\nsns.heatmap(df_curve1[['t', 'rolling_mean']].corr(), annot=True, vmax=1, vmin=-1);\n\nplt.plot(df_curve1.y, c='blue', label='Исходный ряд', alpha=0.5, linestyle=':')\nplt.plot(df_curve1.rolling_mean, c='orange', label='Скользящее среднее', linewidth=3)\n\n\n# графики приростов для выбора наилучшей кривой\ndf_curve1['delta1'] = (df_curve1.rolling_mean.shift(-1) - df_curve1.rolling_mean.shift(1)) / 2\n\ndf_curve1['delta2'] = (df_curve1.delta1.shift(-1) - df_curve1.delta1.shift(1)) / 2\n\ndf_curve1['delta1_div_y'] = df_curve1.delta1 / df_curve1.y\n\ndf_curve1['ln_delta1'] = np.log(df_curve1.delta1)\n\ndf_curve1['ln_delta1_div_y'] = np.log(df_curve1.delta1_div_y)\n\ndf_curve1['ln_delta1_div_y2'] = np.log(df_curve1.delta1_div_y / df_curve1.y)\n\ndf_curve1.head(10)\n\nplt.figure(figsize=(14, 7))\nplt.suptitle('Графики средних приростов')\n\nfor i, col in enumerate(df_curve1.columns[3:]):\n    plt.subplot(2, 3, i + 1)\n    plt.title(col)\n    plt.ylim(-15, 15)\n    plt.grid(alpha=0.5, linestyle='--')\n    plt.plot(df_curve1.t, df_curve1[col], c='red')\n\n# выбор кривой роста\n\n# | Показатель | Характер изменений | Кривая роста |\n# | --- | --- | --- |\n# | $\\Delta y_t$ | Примерно постоянный | Полином первого порядка |\n# | $\\Delta y_t$ | Примерно линейный | Полином второго порядка |\n# | $\\Delta^2 y_t$ | Примерно линейный | Полином третьего порядка |\n# | $\\frac{\\Delta y_t}{y_t}$ | Примерно постоянный | Экспонента |\n# | $\\ln \\Delta y_t$ | Примерно линейный | Модифицированная экспонента |\n# | $\\ln \\frac{\\Delta y_t}{y_t}$ | Примерно линейный | Кривая Гомперца |\n# | $\\ln \\frac{\\Delta y_t}{y^2_t}$ | Примерно линейный | Логистическая кривая |\n\n# если требуется, линеаризуем модель, строим её и возвращаемся к исходному виду\n\ncurve = sm.OLS(np.log(df_curve1.rolling_mean), sm.add_constant(df_curve1.t)).fit()\n\ncurve.summary()\n\n# визуализация\nplt.plot(df_curve1.t, np.exp(curve.predict(sm.add_constant(df_curve1.t))), c='green', label='Кривая роста', linewidth=3)\n\n# точечный прогноз\n\nt_predict = [56, 57, 58, 59]\n\ny_pred = np.exp(curve2.predict(sm.add_constant(t_predict)))\n\n# интервальный прогноз. Для этого посчитаем отклонение от точечного:\n\n# $U(k) = S_{\\hat y} t_\\alpha \\sqrt{1 + \\vec{x}_0^T (X^T X)^{-1} \\vec{x}_0}$,\n\n# где $S_{\\hat y} = \\sqrt{\\frac{\\sum_{t=1}^{n} \\varepsilon_t^2}{n-m-1}}$ и $\\vec{x}_0$ - вектор прогнозных оценок регрессоров\n\nSy = np.sqrt(sum(curve2.resid ** 2) / (len(df[:-4]) - 1 - 1))\n\nfrom scipy.stats import t\nta = t(len(df[:-4] - 2)).isf(0.05)\n\nx0 = sm.add_constant(t_predict)\n\nX = np.array(sm.add_constant(df_curve2.t))\nXT_X = np.linalg.inv(X.T @ X)\n\nUks = []\nfor t_pred in x0:\n    Uks.append(Sy * ta * np.sqrt(1 + t_pred.T @ XT_X @ t_pred))\nUks = np.array(Uks)\n\n# интервалы\ndown = np.exp(curve2.predict(sm.add_constant(t_predict)) - Uks)\nup = np.exp(curve2.predict(sm.add_constant(t_predict)) + Uks)",
    "Модель Брауна": "\n# Модель Брауна\n\n# поиск начальных параметров A0, A1\nfirst24 = sm.OLS(df[:24].y, sm.add_constant(df[:24].t)).fit()\nfirst24.summary()\n\n# оптимальное beta подбором\nmodel_params = [[131.7152, 2.0508]] * 24\nbeta = 0.85\n\nfor i in range(24, len(df)):\n    y_pred = model_params[-1][0] + model_params[-1][1] * df.iloc[i].t\n\n    error = df.iloc[i].y - y_pred\n    \n    temp = [0, 0]\n    temp[0] = model_params[-1][0] + model_params[-1][1] + (1 - beta) ** 2 * error\n    temp[1] = model_params[-1][1] + (1 - beta) ** 2 * error\n\n    model_params.append(temp)\n\nmodel_params = np.array(model_params)\ny_pred = model_params[:, 0] + model_params[:, 1] * df.t\n\ndef find_best_beta(data_offset=None):\n    res = []\n    \n    for beta in np.linspace(0, 1, 10000):\n        first24 = sm.OLS(df[:24].y, sm.add_constant(df[:24].t)).fit()\n\n        model_params = [list(first24.params.values)] * 24\n\n        for i in range(24, len(df[:data_offset])):\n            y_pred = model_params[-1][0] + model_params[-1][1] * df.iloc[i].t\n\n            error = df.iloc[i].y - y_pred\n            \n            temp = [0, 0]\n            temp[0] = model_params[-1][0] + model_params[-1][1] + (1 - beta) ** 2 * error\n            temp[1] = model_params[-1][1] + (1 - beta) ** 2 * error\n\n            model_params.append(temp)\n\n        model_params = np.array(model_params)\n\n        y_pred = model_params[:, 0] + model_params[:, 1] * df.t[:data_offset]\n\n        res.append(np.sum((df.y[:data_offset] - y_pred) ** 2))\n\n    return res\n    \nresults = find_best_beta()\nnp.argmin(results) / 10000, min(results)\n\n# прогнозные значения и их интервалы\nSy = y_pred.std()\nn = len(df)\nta = t(n - 2).isf(0.05)\nUk = lambda k: Sy * ta * np.sqrt(1 + 1 / n + 3 * (n + 2 * k - 1) ** 2 / n / (n ** 2 - 1))\nUks = Uk(np.arange(60, 64))\ny_pred_new = model_params[-1, 0] + model_params[-1, 1] * np.arange(60, 64)\ndown = y_pred_new - Uks\nup = y_pred_new + Uks\n\ndown, y_pred_new, up\n\n# c использование 4 последних точек, как валидации\n\nresults = find_best_beta(data_offset=-4)\n\n#rebuild model with offset\n\nSy = y_pred.std()\nn = len(df) - 4\nta = t(n - 4 - 2).isf(0.05)\nUk = lambda k: Sy * ta * np.sqrt(1 + 1 / n + 3 * (n + 2 * k - 1) ** 2 / n / (n ** 2 - 1))\nUks = Uk(np.arange(56, 60))\ny_pred_new = model_params[-1, 0] + model_params[-1, 1] * np.arange(56, 60)\ndown = y_pred_new - Uks\nup = y_pred_new + Uks\n\ndown, y_pred_new, up\n",
    "Модель Хольта-Уинтерса": "\n# Модель Хольта-Уинтерса\n\ndef HolterWinter(t, y, alpha, beta, gamma, L, data_offset=None):\n    a = [y[0]]\n    b = [0]\n    F = [1] * L\n\n    for i in range(1, L):\n        a_temp = alpha * (y[i] / 1) + (1 - alpha) * (a[i - 1] + b[i - 1])\n        b_temp = beta * (a_temp - a[i - 1]) + (1 - beta) * b[i - 1]\n\n        a.append(a_temp)\n        b.append(b_temp)\n    \n    for i in range(L, len(y[:data_offset])):\n        a_temp = alpha * (y[i] / F[i - L]) + (1 - alpha) * (a[i - 1] + b[i - 1])\n        b_temp = beta * (a_temp - a[i - 1]) + (1 - beta) * b[i - 1]\n        F_temp = gamma * (y[i] / a_temp) + (1 - gamma) * F[i - L]\n\n        a.append(a_temp)\n        b.append(b_temp)\n        F.append(F_temp)\n\n    return map(np.array, [a, b, F])\n\n# с валидационной выборкой\n\nalpha, beta, gamma = 0.1, 0.1, 0.9\n\na, b, F = HolterWinter(df.t, df.y, alpha, beta, gamma, 4, data_offset=-4)\n\ny_pred = (a + b) * F\n\nk = np.arange(1, 5)\n\ny_pred_new = (a[-1] + b[-1] * k) * F[-5 + (k % 4)]\n\nplt.figure(figsize=(10, 5))\nplt.title(f'Модель Холтера-Уинторса ({alpha=}, {beta=}, {gamma=})')\nplt.grid(alpha=0.5, linestyle='--')\nplt.xlabel('t, период времени')\nplt.ylabel('y')\nplt.plot(df.t[-6:-4] + 1, [df.y.iloc[-5], y_pred_new[0]], c='lightgreen', linewidth=3)\nplt.plot(df.t[:-4], y_pred, c='orange', label='Модель Холтера-Уинторса', linewidth=3)\nplt.plot(df.t[-4:], y_pred_new, c='lightgreen', label='Прогноз', linewidth=3, marker='o', markerfacecolor='pink')\nplt.plot(df.t, df.y, c='blue', label='Исходный ряд', alpha=0.5, linestyle=':')\nplt.legend(loc='upper left');\n\n# экстраполяция вперёд\n\nalpha, beta, gamma = 0.179, 0.1, 0.762\n\na, b, F = HolterWinter(df.t, df.y, alpha, beta, gamma, 4)\n\ny_pred = (a + b) * F\n\nk = np.arange(1, 5)\n\ny_pred_new = (a[-1] + b[-1] * k) * F[-5 + (k % 4)]\n\ny_pred_new\n",
    "метод Четверикова": "\n# метод Четверикова\n\n\ndef Chetverikov(t, y, L):\n    y_reshaped = np.array(y).reshape(-1, L)\n    chrono_mean = (\n        0.5 * y_reshaped[:, 0]\n        + np.sum(y_reshaped[:, 1:-1], axis=1)\n        + 0.5 * y_reshaped[:, -1]\n    ) / L\n    l1 = y_reshaped - chrono_mean[[[i] * 4 for i in range(len(chrono_mean))]]\n    l1_2 = l1**2\n    sigma1 = np.sqrt((np.sum(l1_2, axis=1) - np.sum(l1, axis=1) ** 2 / L) / (L - 1))\n    l1_norm = l1 / sigma1[[[i] * 4 for i in range(len(chrono_mean))]]\n    s1 = np.mean(l1_norm, axis=0)\n\n    f1 = (\n        y_reshaped\n        - s1[[list(range(0, L)) for i in range(len(y_reshaped))]]\n        * sigma1[[[i] * 4 for i in range(len(chrono_mean))]]\n    )\n\n    f2 = [np.nan] * (int(0.5 * L) - 1)\n\n    for i in range(int(0.5 * L), len(f1.reshape(-1)) + 1):\n        f2.append(np.mean(f1.reshape(-1)[i - int(0.5 * L) : i]))\n\n    f2[0] = f1[0, 0]\n\n    f2 = np.array(f2).reshape(-1, L)\n\n    l2 = y_reshaped - f2\n\n    l2_2 = l2**2\n\n    sigma2 = np.sqrt((np.sum(l2_2, axis=1) - np.sum(l2, axis=1) ** 2 / L) / (L - 1))\n    l2_norm = l2 / sigma2[[[i] * 4 for i in range(len(chrono_mean))]]\n\n    s2 = np.mean(l2_norm, axis=0)\n\n    eps = (\n        l2\n        - s2[[list(range(0, L)) for i in range(len(y_reshaped))]]\n        * sigma2[[[i] * 4 for i in range(len(chrono_mean))]]\n    )\n\n    k = np.sum(l2_2 * eps, axis=1) / np.sum(eps**2, axis=1)\n\n    return {\n        \"chrono_mean\": chrono_mean[[[i] * 4 for i in range(len(chrono_mean))]].reshape(\n            -1\n        ),\n        \"f1\": f1.reshape(-1),\n        \"f2\": f2.reshape(-1),\n        \"s1\": s1,\n        \"s2\": s2,\n        \"eps\": eps.reshape(-1),\n    }\n\n\nres = Chetverikov(df.t[:-3], df.y[:-3], 4)\n\n# диаграммы тренды\n\nplt.figure(figsize=(8, 8))\nplt.suptitle(\"Диаграммы тренда\")\nplt.subplots_adjust(hspace=0.5)\n\ntitles = [\n    \"Исходный ряд\",\n    \"Предварительная оценка тренда\",\n    \"Первая оценка тренда\",\n    \"Вторая оценка тренда\",\n]\n\nfor i, col in enumerate([df.y[:-3], res[\"chrono_mean\"], res[\"f1\"], res[\"f2\"]]):\n    plt.subplot(4, 1, i + 1)\n    plt.title(titles[i])\n    if i == 3:\n        plt.xlabel(\"t, период времени\")\n    if i:\n        plt.ylim(90, 150)\n    else:\n        plt.ylim(50, 300)\n    plt.grid(alpha=0.5, linestyle=\"--\")\n    plt.plot(df.t[:-3], col, c=(\"orange\" if i else \"blue\"))\n\n# данные о сезонности\n\nplt.figure(figsize=(10, 7))\nplt.suptitle(\"Диаграммы сезонной волны\")\nplt.subplots_adjust(hspace=0.5)\n\n\ntitles = [\"Первая оценка сезонности\", \"Вторая оценка сезонности\", \"Сравнение\"]\n\nfor i, col in enumerate([res[\"s1\"], res[\"s2\"], [res[\"s1\"], res[\"s2\"]]]):\n    plt.subplot(3, 1, i + 1)\n    plt.title(titles[i])\n    if i == 2:\n        plt.xlabel(\"L, квартал\")\n    plt.grid(alpha=0.5, linestyle=\"--\")\n    if i < 2:\n        plt.plot(np.arange(1, 5), col, c=(\"red\" if i else \"lightgreen\"), linewidth=1)\n    else:\n        plt.plot(np.arange(1, 5), col[0], c=\"red\", linewidth=1)\n        plt.plot(np.arange(1, 5), col[1], c=\"lightgreen\", linewidth=1)\n\n# диаграмма остаточной компоненты\n\nplt.figure(figsize=(10, 5))\nplt.title(\"Остаточная компонента\")\nplt.grid(alpha=0.5, linestyle=\"--\")\nplt.xlabel(\"t, период времени\")\nplt.ylabel(\"e\")\nplt.ylim(-20, 20)\nplt.xlim(-1, 57)\nplt.hlines(0, -5, 70, color=\"black\", linewidth=0.4)\nplt.plot(\n    df.t[:-3], res[\"eps\"], c=\"lightblue\", label=\"Остаточная компонента\", linewidth=3\n)\nplt.legend(loc=\"upper left\");\n\n"
  },
  "3. Логит и пробит (мб хэкит и пробит)": {
    "Логит модель": "\n# логит модель\n\nlogit_model = sm.Logit(y, X).fit()\nlogit_model.summary()\n\n# спецификация\n# $ \\huge Y^* =  \\frac{1}{1 + e^{-4.3377\t(1.615) + 0.8840(0.401) \\cdot x_0 -0.1353(0.455) \\cdot x_1 + 0.3341(0.256) \\cdot x_2}} $\n\n# Расчет средних предельных эффектов (значимость факторов)\n# Это изменение вероятности исхода при изменении предиктора на единицу, с учетом среднего значения всех других предикторов.\nmarginal_effects = logit_model.get_margeff(at='mean', method='dydx')\nprint(marginal_effects.summary())\n",
    "Пробит модель": "\n# пробит модель\n\nprobit_model = sm.Probit(y, X).fit()\nprobit_model.summary()\n\n# спецификация\n# $ Y^* =  \\Phi(- 2.5312(0.894) + 0.4945(0.215) \\cdot x_0 - 0.0650(0.26) \\cdot x_1 + 0.1952(0.154) \\cdot x_2) $\n\n# Расчет средних предельных эффектов (значимость факторов)\n# Это изменение вероятности исхода при изменении предиктора на единицу, с учетом среднего значения всех других предикторов.\nmarginal_effects = probit_model.get_margeff(at='mean', method='dydx')\nprint(marginal_effects.summary())\n",
    "Хекит модель": "\n# хекит модель\nhttps://github.com/statsmodels/statsmodels/blob/92ea62232fd63c7b60c60bee4517ab3711d906e3/statsmodels/regression/heckman.py\n\n# NaN если цензурируется\nY_cens = Y.copy()\nY_cens[Y_cens == 800] = np.NaN\ncens = pd.Series(np.ones((len(Y),))).astype(int)\n\n#разделение экзогенных переменных на имеющие непрерывное (Xh) и дискретное распределение (Sh)\n\nSh = X.iloc[:, -2:]\nXh = X.iloc[:, :-2]\n\nimport heckman as heckman\nheckman_model = heckman.Heckman(Y_cens_train, X_train, cens_train)\nres_h = heckman_model.fit(method='twostep')\nprint(res_h.summary())\n\n# первые коэффициенты - Response equation (Модель линейной регрессии)\n\n# вторые коэффициенты - Selection equation (Модель выбора - пробита)\n# спецификация записывается как Phi(a_0*Z_0 + a_1*Z_1 _ eps)\n",
    "Тобит модель": "\n# тобит модель\nhttps://github.com/jamesdj/tobit\n\n# Здесь, чтобы обозначить левостороннее цензурирование, передаем в вектор cens -1 (в нашем случае такого нет, т.к. нет наблюдений == 200),\n# для правостороннего передаем 1, а если не цензурировалось - 0.\n\ncens[Y == 800] = 1\n\nfrom tobit import *\n\ntobit_model = TobitModel()\ntobit_model.fit(sm.add_constant(X_train), Y_train, cens)\n\ntobit_model.coef_\n"
  },
  "4. Стационарные ряды": {
    "Тест Дики-Фуллера": "\nfrom statsmodels.tsa.stattools import adfuller\nadfuller(data)\n\n#если p-value > 0.05$, то нулевая гипотеза не отвергается и ряд не является стационарным.\n# => преобразование \n\ndiff = (data.shift(1) - data).iloc[1:]\n\n# и снова\nadfuller(diff)\n# и снова преобрзаование, если ряд не стал стационарным\n",
    "Модели": {
      "AR": "\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.ar_model import AutoReg\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n\n# # Создание фиктивного DataFrame с временными рядами\n# data = {\n#     'date': pd.date_range(start='2020-01-01', periods=100, freq='M'),\n#     'value': np.random.normal(loc=100, scale=10, size=100)\n# }\n# df = pd.DataFrame(data)\n# df.set_index('date', inplace=True)\n\n# Визуализация временного ряда\nplt.figure(figsize=(10, 6))\nplt.plot(df['value'])\nplt.title('Time Series')\nplt.show()\n\n# Построение ACF и PACF для выбора параметра p\nfig, ax = plt.subplots(2, 1, figsize=(12, 8))\nplot_acf(df['value'], lags=20, ax=ax[0])\nplot_pacf(df['value'], lags=20, ax=ax[1])\nplt.show()\n\n#автоматически\n                     \nfrom statsmodels.tsa.stattools import acf, pacf\n\nvalues_acf, confint_acf = acf(x_t.squeeze(), nlags=5, alpha=0.05)\nvalues_pacf, confint_pacf = pacf(x_t.squeeze(), nlags=5, method='ywm', alpha=0.05)\n\nfor i in range(5):\n    if confint_acf[i][0] // 1 == confint_acf[i][1] // 1:\n        print(f\"{i} по ACF значимо\")\n\nfor i in range(5):\n    if confint_pacf[i][0] // 1 == confint_pacf[i][1] // 1:\n        print(f\"{i} по PACF значимо\")\n\n\n# Выбор параметра p\np = 3  # Например, выбираем p = 3 на основе PACF\n\n# Построение модели\nmodel = AutoReg(df['value'], lags=p).fit()\n\n# Вывод параметров модели\nprint(model.summary())\n\n# Прогнозирование\nforecast_steps = 12\nforecast = model.predict(start=len(df), end=len(df) + forecast_steps - 1, dynamic=False)\n\n# Визуализация прогноза\nplt.figure(figsize=(10, 6))\nplt.plot(df['value'], label='Original Series')\nplt.plot(pd.date_range(start=df.index[-1], periods=forecast_steps + 1, freq='M')[1:], forecast, label='Forecast', color='red')\nplt.title('AR Model Forecast')\nplt.legend()\nplt.show()\n",
      "ARMA": "\n#графики\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n\nplot_acf(x_t.squeeze(), lags=40, alpha=0.05)\nplt.title(\"ACF (AutoСorrelation Function)\")\n\nplot_pacf(x_t.squeeze(), lags=40, alpha=0.05, method=\"ywm\")\nplt.title(\"PACF (Partial AutoСorrelation Function)\")\n\n#автоматическая проверка значимости\nfrom statsmodels.tsa.stattools import acf, pacf\n\nvalues_acf, confint_acf = acf(x_t.squeeze(), nlags=5, alpha=0.05)\nvalues_pacf, confint_pacf = pacf(x_t.squeeze(), nlags=5, method='ywm', alpha=0.05)\n\nfor i in range(5):\n    if confint_acf[i][0] // 1 == confint_acf[i][1] // 1:\n        print(f\"{i} по ACF значимо\")\n\nfor i in range(5):\n    if confint_pacf[i][0] // 1 == confint_pacf[i][1] // 1:\n        print(f\"{i} по PACF значимо\")\n\nfrom statsmodels.tsa.arima.model import ARIMA\n\nmodel = ARIMA(x_t, order=(1, 0, 2)) #d=0 => ARMA\nresult = model.fit()\nprint(result.summary())\n\n# прогноз\nforecast = result.get_forecast(steps=10)\nforecast_summary = forecast.summary_frame()\n\nplt.figure(figsize=(18, 6))\nplt.plot(x_t, label='Известные значения')\nplt.plot(forecast_summary['mean'], label='Прогноз', color='red')\nplt.fill_between(\n    forecast_summary.index,\n    forecast_summary['mean_ci_lower'],\n    forecast_summary['mean_ci_upper'],\n    color='pink',\n    alpha=0.3,\n    label='Доверительный интервал прогноза',\n)\nplt.title('Ежемесячная доходность корпоративных облигаций класса Aaa')\nplt.xlabel('Время')\nplt.ylabel('Доходность')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n#качество\nm.r2_score(x_t[:-1], result.predict()[1:])\n",
      "ARIMA": "\n#графики\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n\nplot_acf(x_t.squeeze(), lags=40, alpha=0.05)\nplt.title(\"ACF (AutoСorrelation Function)\")\n\nplot_pacf(x_t.squeeze(), lags=40, alpha=0.05, method=\"ywm\")\nplt.title(\"PACF (Partial AutoСorrelation Function)\")\n\n#автоматическая проверка значимости\nfrom statsmodels.tsa.stattools import acf, pacf\n\nvalues_acf, confint_acf = acf(x_t.squeeze(), nlags=5, alpha=0.05)\nvalues_pacf, confint_pacf = pacf(x_t.squeeze(), nlags=5, method='ywm', alpha=0.05)\n\nfor i in range(5):\n    if confint_acf[i][0] // 1 == confint_acf[i][1] // 1:\n        print(f\"{i} по ACF значимо\")\n\nfor i in range(5):\n    if confint_pacf[i][0] // 1 == confint_pacf[i][1] // 1:\n        print(f\"{i} по PACF значимо\")\n\nfrom statsmodels.tsa.arima.model import ARIMA\n\n# берём x_t = (df.AAA.shift(1) - df.AAA).dropna()\n# пока не станет стационарным или же указываем нужное d\nmodel = ARIMA(x_t, order=(1, d, 2)) #d=0 => ARMA\nresult = model.fit()\nprint(result.summary())\n\n# прогноз\nforecast = result.get_forecast(steps=10)\nforecast_summary = forecast.summary_frame()\n\nplt.figure(figsize=(18, 6))\nplt.plot(x_t, label='Известные значения')\nplt.plot(forecast_summary['mean'], label='Прогноз', color='red')\nplt.fill_between(\n    forecast_summary.index,\n    forecast_summary['mean_ci_lower'],\n    forecast_summary['mean_ci_upper'],\n    color='pink',\n    alpha=0.3,\n    label='Доверительный интервал прогноза',\n)\nplt.title('Ежемесячная доходность корпоративных облигаций класса Aaa')\nplt.xlabel('Время')\nplt.ylabel('Доходность')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n#качество\nm.r2_score(x_t[:-1], result.predict()[1:])\n"
    }
  },
  "5. Системы уравнений": {
    "Общие понятия": "\n# пример переменных\na1, a2, b1, b2, SBER_t, VTB_t, USD_t, t, u_t, v_t = sp.symbols(\n    'a1, a2, b1, b2, SBER_t, VTB_t, USD_t, t, u_t, v_t')\n\n# эндогенные\nY = sp.Matrix([SBER_t, VTB_t])\n\n# экзогенная\nX = sp.Matrix([USD_t, t])\n\n# случайные возмущения\nU = sp.Matrix([u_t, v_t])\n\n# $\\begin{cases}\n# \\text{SBER}_t = a_1 t + a_2 \\text{VTB}_t + u_t \\\\\n# \\text{VTB}_t = b_1 \\text{USD}_t + b_2 \\text{SBER}_t + v_t\n# \\end{cases}$\n# исходя из вида системы\nA = sp.Matrix([[1, -a2], [-b2, 1]])\n\nB = sp.Matrix([[0, -a1], [-b1, 0]])\n\n# приведённая форма\n\nY_priv = -(A**-1) * B * X + A**-1 * U\n",
    "Проверки идентифицируемости": {
      "Правило ранга": "\n# A, B расписаны в Общие понятия\nA_full = sp.Matrix.hstack(A, B)\n\ndef get_r_matrix(row):\n    zero_indices = [i for i, value in enumerate(row) if value == 0]\n\n    r = sp.zeros(len(zero_indices), len(row))\n    for i, index in enumerate(zero_indices):\n        r[i, index] = 1\n\n    return r\n\nfor i in range(A_full.shape[0]):\n    print((A_full * get_r_matrix(A_full[i, :]).T).rank())\n\n#все значения должны быть равны m-1, где m - кол-во уравнений\n",
      "Правило порядка": "\nПравило порядка (необходимое условие):\n\nD - число экзогенных уравнений в системе, но не в уравнении\n\nH - число эндогенных переменных в уравнении\n\n1. Если D + 1 = H - точно идентифицируемо\n2. Если D + 1 < H - неидентифицируемо\n3. Если D + 1 > H - сверхидентифицируемо\n"
    },
    "Методы оценки моделей": {
      "КМНК": "\n# если все уравнения ТОЧНО идентифицируемы\n\n# оцениваем каждое уравнение\nmodel = sm.OLS(df.SBER, np.array([df.USD, np.array(df.index)]).T)\nresult1 = model.fit()\n\nmodel = sm.OLS(df.VTB, np.array([df.USD, np.array(df.index)]).T)\nresult2 = model.fit()\n\n# объединяем полученные регрессии\nM = sp.Matrix([list(result1.params), list(result2.params)])\nME = sp.Matrix.vstack(M, sp.eye(A_full.shape[1] - M.shape[0]))\n\n# возвращаемся к исходным параметрам\nfinal_params = {}\nfor i in range(A_full.shape[0]):\n    for k, v in sp.solve(A_full[i, :] * ME).items():\n        final_params[k] = v\n",
      "2-МНК": "\nfrom linearmodels import IV2SLS\n# если все уравнения идентифицируемы\nmodel1 = IV2SLS(\n    dependent=df['SBER'], exog=df[['t']], endog=df['VTB'], instruments=df['USD']\n).fit()\n\nmodel1.params\n\nmodel2 = IV2SLS(\n    dependent=df['VTB'], exog=df[['USD']], endog=df['SBER'], instruments=df['t']\n).fit()\n\nmodel2.params\n\n#обычно, оценки совпадают с кмнк\n",
      "3-МНК": "\nfrom linearmodels import IV3SLS\n# если все уравнения идентифицируемы\n# учитывает автокорреляцию между уравнениями\n\neq1 = {\n    'dependent': df['SBER'],\n    'exog': df[['t']],\n    'endog': df[['VTB']],\n    'instruments': df[['USD']],\n}\n\n\neq2 = {\n    'dependent': df['VTB'],\n    'exog': df[['USD']],\n    'endog': df[['SBER']],\n    'instruments': df[['t']],\n}\n\nsystem = IV3SLS({'SBER': eq1, 'VTB': eq2})\n\nresults = system.fit()\n\nresults.params\n\n# или же\n\neq1 = \"Y ~ C + I + G + NX\"\neq2 = \"C ~ 1 + Y_minus_T\"\neq3 = \"I ~ 1 + r\"\neq4 = \"NX ~ 1 + e\"\neq5 = \"CF ~ 1 + r\"\neq6 = \"NX ~ CF_neg\"\n\nformula = {str(x): x for x in [eq1, eq2, eq3, eq4, eq5, eq6]}\n\nmod = IV3SLS.from_formula(\n    formula,\n    data=train_df,\n)\n\nresults = mod.fit()\n\nresults\n",
      "SUR": "\nfrom linearmodels import SUR\n# если все уравнения идентифицируемы\n\neq1 = {\"dependent\": df[\"SBER\"], \"exog\": df[[\"t\", \"USD\"]]}\neq2 = {\"dependent\": df[\"VTB\"], \"exog\": df[[\"t\", \"USD\"]]}\n\nsystem = SUR({\"SBER\": eq1, \"VTB\": eq2})\n\nresults = system.fit()\n\nprint(results.params)\n"
    }
  },
  "6. Динамические модели": {
    "H-тест": "\n# коеф дарбина-уотсона\ndb = float(model_koika.summary().tables[2][0][3].data)\n\nro_hat = 1 - db / 2\nn = new_data.shape[0]\nD_hat = model_koika.bse[-1] ** 2\nh = ro_hat * np.sqrt(n / (1 - n * D_hat))\n\nalpha = 0.05\nh_cr = scs.norm().ppf(1 - alpha/2)\n\nabs(h) > h_cr #если True, то гипотеза о независимости остатков отклоняется и присутствует автокорреляция\n",
    "Модели": {
      "Метод геометрической прогрессии": "\ndef geom_profression_method(X, y, delta, lambd_n=100, max_lag=20):\n    best_rsquared = 0\n    best_model = None\n\n    for lambd in tqdm.tqdm(np.linspace(0, 1, lambd_n + 1)):\n        X_ = X.copy()\n        y_ = y.copy()\n        p = 0\n        z = [sum(np.mean(X_, axis=0))]\n        while True:\n            p += 1\n            temp = X_[p:]\n            for i in range(0, p + 1):\n                temp += lambd**i * X_.shift(i).dropna()\n            z.append(sum(np.mean(temp, axis=0)))\n            if np.abs((z[-1] - z[-2]) / z[-2]) < delta or p >= max_lag:\n                break\n\n        model = sm.OLS(y[temp.index], sm.add_constant(temp)).fit()\n        if model.rsquared > best_rsquared:\n            best_rsquared = model.rsquared\n            best_model = model\n            p_best = p\n            lambd_best = lambd\n\n    return best_model, p_best, lambd_best\n\ny = all_data.Close\nX = all_data[[\"psr\", \"m2\"]]\n\ndelta = 0.005\n\nbest_model, best_p, best_lambd = geom_profression_method(X, y, delta)\nbest_model.summary()\n",
      "Преобразование Койка": "\n# $ Y_t = (1 - \\lambda) a_0 + b_0 \\cdot X_t + \\lambda \\cdot Y_{t-1} + \\varepsilon_t - \\lambda \\cdot \\varepsilon_{t-1} $\n\nnew_data = pd.concat([X, y.shift(1)], axis=1).dropna()\nnew_data.columns.values[4] = \"Close_{t-1}\"\n\nmodel_koika = sm.OLS(y, sm.add_constant(X)).fit()\n\nb0 = round(model_koika.params[\"psr\"], 4)\nb1 = round(model_koika.params[\"m2\"], 4)\nlambd = round(model_koika.params[\"Close_{t-1}\"], 4)\na0 = round(model_koika.params.const / (1 - lambd), 4)\nb0, b1, lambd, a0\n\n# модель с бесконечными лагами: a0 + b1 * lambd**p для p от 0 до inf\n",
      "Модель адаптивных ожиданий": "\n# Оценивается следующий вид модели:\n# $Y_t = a + b X^*_t + \\varepsilon_t$\n# С помощью преобразования можем прийти к удобному виду:\n# $Y_t = \\gamma a + \\gamma b X_t + (1 - \\gamma) Y_{t - 1} + u_t$\n\nmodel_koika = sm.OLS(y, sm.add_constant(X)).fit()\ngamma = round(1 - model_koika.params[\"Close_{t-1}\"], 4)\nb0 = round(model_koika.params[\"psr\"] / gamma, 4)\nb1 = round(model_koika.params[\"m2\"] / gamma, 4)\na0 = round(model_koika.params.const / gamma, 4)\na0, b0, b1, gamma\n",
      "Модель частичных корректировок": "\n# Необходимо построить следующую модель:\n# $Y^*_t = a + b X_t + \\varepsilon_t$\n# При помощи преобразований можем осуществить оценку параметров:\n# $Y_t = \\lambda a + (1 - \\lambda) Y_{t-1} + \\lambda b X_t + \\lambda \\varepsilon_t$\n\nmodel_koika = sm.OLS(y, sm.add_constant(X)).fit()\nlambd = round(1 - model_koika.params[\"Close_{t-1}\"], 4)\nb0 = round(model_koika.params[\"psr\"] / lambd, 4)\nb1 = round(model_koika.params[\"m2\"] / lambd, 4)\na0 = round(model_koika.params.const / lambd, 4)\na0, b0, b1, gamma\n",
      "Лаги Алмон": "\n# Для построение модели с использованием лагов Алмон, необходимо вычислить следующие величины:\n# $Z_{tp} = \\sum_{i = 0}^{k} {i^p X_{t-i}}$\n# где $p$ - степень полинома, $k$ - количество лагов\n\ndef get_almon(X, p, k):\n    Z = []\n\n    data = pd.DataFrame(X.copy())\n    for i in range(1, k + 1):\n        data[f'X{i}'] = X.shift(i)\n\n    data = data.dropna()\n\n    for j in range(p):\n        temp = 0\n        for i in range(1, k + 1):\n            temp += i ** j * data.iloc[:, i]\n        Z.append(temp)\n\n    result = pd.concat(Z, axis=1)\n\n    result.columns = [f\"X{i}\" for i in range(1, p + 1)]\n\n    return result\n\nk = 14\np = 3\n\nz = get_almon(X, p, k)\n\n# Оценим следующую модель:\n# $Y_t = a_0 + \\nu_0 Z_{t0} + \\nu_1 Z_{t1} + \\nu_2 Z_{t2} + \\epsilon_t$\n\nmodel = sm.OLS(y[k:], sm.add_constant(z)).fit()\n\n#модель лагов алмон\nnu1 = model.params.X1\nnu2 = model.params.X2\nnu3 = model.params.X3\n\n# или изначальный вид\ncoefs = [a0] + [nu1 + nu2 * i + nu3 * i ** 2 for i in range(k + 1)] + [None]\nstds  = [model.bse.const] + [None] * (k + 1)\nvars  = [None, 'X_t'] + [f'X_{{t - {i}}}' for i in range(1, k + 1)] + ['\\\\varepsilon_t']\n"
    }
  },
  "7. Панельные данные": {
    "Тесты": {
      "Pooled vs FE (тест Фишера)": "\n# $F = \\frac{(RSS_P - RSS_{FE}) / (n - 1)}{RSS_{FE} / (nT - n - k)}$\n# $F_{крит} = F_\\alpha(n - 1; nT - n - k)$\n\nfrom scipy.stats import f\n\nRSSp = pooled.resid_ss\nRSSfe = FE.resid_ss\n\nT = data.reset_index().Year.nunique()\nk = len(X.columns)\n\nn = len(y) / T\nF = (RSSp - RSSfe) / (n - 1) / (RSSfe / (n * T - n - k))\nF_cr = f(n - 1, n * T - n - k).isf(0.95)\n\n# если $F > F_{крит} \\Rightarrow H_0$ отвергается, $\\text{FE}$ лучше $\\text{Pooled}$. \n",
      "Pooled vs RE (тест Брауша-Пагана)": "\nimport statsmodels.stats.diagnostic as dg\n\n_, _, _, pv = dg.het_breuschpagan(pooled.resids, sm.add_constant(pooled.model.exog.dataframe))\n\n# если pv < alpha, то остатки не гомоскедастичны (так как отвергается H_0), а значит RE лучше Pooled$.\n",
      "FE vs RE (тест Хаусмана)": "\nimport numpy.linalg as la \nfrom scipy import stats  \n\ndef hausman(fe, re): \n    b = fe.params     \n    B = re.params   \n    v_b = fe.cov     \n    v_B = re.cov     \n    df = b.size     \n    chi2 = np.dot((b - B).T, la.inv(v_b - v_B).dot(b - B))    \n    pval = stats.chi2.sf(chi2, df)     \n    return chi2, pval\n\nhausman(FE, RE)\n\n# если p-value < alpha, то есть достаточно высокого значения статистики хи-квадрат, \n# оно попадает в критическую область и H_0 отвергается, \n# поэтому между RE и FE лучше оказывается FE\n"
    },
    "Модели": {
      "Pooled Regression": "\nfrom linearmodels import PooledOLS\n\ndata = data.set_index(['Reg', 'Year'])\n\npooled = PooledOLS(y, X).fit()\npooled.summary\n\n# спецификация как лин рег\n",
      "Fixed Effect Model": "\nfrom linearmodels import PanelOLS\n\ndata = data.set_index(['Reg', 'Year'])\n\nFE = PanelOLS(y, X, entity_effects=True).fit()\nFE.summary\n\n#коэффы областей\nFE.estimated_effects\n\n# спецификация как лин рег + FE.estimated_effects\n",
      "Random Effect Model": "\nfrom linearmodels import RandomEffects\n\ndata = data.set_index(['Reg', 'Year'])\n\nRE = RandomEffects(y, X).fit()\nRE.summary\n\n#коэффы областей\nRE.estimated_effects\n\n# спецификация как лин рег + RE.estimated_effects + u (сам случайный эффект)\n"
    }
  },
  "8. Информационные критерии": {
    "Критерий Акаике": "\ndef AIC_criterion(data):\n    ess =sum(sm.OLS(data['Y'], data[['const', 'X']]).fit().resid**2)\n\n    return np.log(ess/len(data))+2/len(data)+1+np.log(2*np.pi)",
    "Критерий Шварца": "\ndef BIC_criterion(data):\n    ess =sum(sm.OLS(data['Y'], data[['const', 'X']]).fit().resid**2)\n    \n    return np.log(ess/len(data))+np.log(len(data))/len(data)+1+np.log(2*np.pi)"
  },
  "9. Модели бинарного выбора": {
    "Метод Зарембки": "\n#метод Зарембки\n\ndata = sm.add_constant(data)\ndata1 = data.copy()\ndata1['y'] = np.log(data['y'])\n\ndef Zarembki(data, data1):\n    y_geom = np.exp(1/len(data) * np.sum(data1['y']))\n    \n    data_norm = data.copy()\n    data1_norm = data1.copy()\n    \n    data_norm['y'] = data_norm['y']/y_geom\n    data1_norm['y'] = np.log(data_norm['y']/y_geom)\n    \n    result = sm.OLS(data_norm['y'], data_norm[['const', 'X']]).fit()\n    result1 = sm.OLS(data1_norm['y'], data1_norm[['const', 'X']]).fit()\n    \n    ess1=sum(result.resid**2)\n    ess2=sum(result1.resid**2)\n    \n    z=abs(len(data)/2*np.log(ess1/ess2))\n    \n    print(f'z: {z}, Хи^2 критическое: {scs.chi2.ppf(0.95,1)}')\n    \n    if z>scs.chi2.ppf(0.95,1):\n        print('Модели имеют разницу, полулогарифмическая модель лучше, так как она лучше описывает зависимости между данными')\n    else:\n        print('Модели имеют разницу, линейная модель лучше, так как она легче')\n\nZarembki(data, data1)\n",
    "Тест Бокса-Кокса": "\n# Тест Бокса-Кокса\n\ndef B_C_test(data, lamda):\n    ess_list = []\n    \n    for i in lamda:\n        data_help=data.copy()\n        data_help['X']=data_help['X']**i/i\n        ess_list.append(sum(sm.OLS(data['y']**i/i, data_help[['const', 'X']]).fit().resid**2))\n        \n    return ess_list\n\nlamda = np.arange(0.001, 1, 0.01)\nplt.plot(lamda, B_C_test(data, lamda))\n\nprint(f'Значение лямбда с наименьшим ESS: {lamda[err.index(min(err))]}')\n\nесли ESS меньше для $\\lambda$ = 0, то сделаем выбор в пользу полулогарифмической модели, иначе в пользу линейной\n"
  }
}