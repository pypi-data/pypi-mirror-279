# AlphaProfit Trading Gym Env

**Trading Gym Env** - это "тренажер", где можно имитировать торговлю активами и обучать торговых роботов с помощью специальных методов RL. Он был разработан для быстрой и настраиваемой реализации алгоритмов RL для торговли.

**Reinforcement Learning** (RL) - это одна из областей машинного обучения, где агент учится принимать решения, выполняя действия в некоторой среде, чтобы максимизировать некоторый кумулятивный выигрыш.

**Вот ключевые понятия и компоненты RL:**

- **Агент (agent)** – сущность, которая принимает решения. В контексте игр это может быть игроком, а в торговле – торговым алгоритмом.
- **Среда (env)** – это всё, с чем агент взаимодействует. Агент делает действия в среде и получает обратную связь от неё.
- **Состояние (state)** – это описание текущей ситуации в среде. Например, в игре шахмат состоянием может быть расположение всех фигур на доске.
- **Действие (action)** – это то, что агент решает сделать в данный момент времени. В торговле это может быть решение купить, продать или удержать актив.
- **Вознаграждение (reward)** – это численное значение, которое агент получает после выполнения действия. Цель агента – максимизировать сумму вознаграждений за определенный период времени.
- **Политика (policy)** – это стратегия, которую агент использует для принятия решений. Это может быть простой алгоритм или сложная нейронная сеть, которая определяет, какое действие выполнить на основе текущего состояния.


RL отличается от других форм машинного обучения тем, что агенту ***не предоставляются "правильные" ответы***. Вместо этого агенту предоставляется обратная связь в виде вознаграждения, основанная на его действиях, и он должен самостоятельно выяснить, какие действия приводят к максимальному вознаграждению.

Применение RL охватывает множество областей: от игр (например, обучение компьютера играть в шахматы или Go) до робототехники, финансов и многого другого.


#### На данный момент доступны следующие агенты: 
- BuyAndHoldAgent
- RandomAgent 
- EqualWeightingAgent
- A2C
- PPO
- DDPG
- TD3
- SAC

## Требования

- Python >=3.10, <3.12
- gym ^0.26.2
- stable-baselines3 ^2.1.0
- loguru ^0.7.0
- matplotlib ^3.7.2
- seaborn ^0.12.2
- ... (и другие зависимости, указанные в `pyproject.toml`)

## Установка

### Через Poetry

1. Установите Poetry:

```
pip install poetry
```

2. Установите зависимости проекта:

```
poetry install
```

### Через Docker

Dockerfile предоставлен для удобства установки и использования проекта в контейнере. Чтобы использовать Docker:

1. Соберите образ:

```
docker build -t trading_gym_env .
```

2. Запустите контейнер:

```
docker run -it trading_gym_env
```

## Использование

Пример использования:

```python
...
from tradegym.tradegym import TradingEnv
from tradegym.agents import *
from tradegym.crossval import BackTest

import tradegym
print(tradegym.__version__)

...

agent = RandomAgent(_, e_test_gym, symbol='BTC-USDT-PERP')
bk = BackTest(e_test_gym, agent, verbose_step=1000)
bk.render()

bk.get_plot_profit_benchmark().show()
...
```

Подробные примеры и эксперименты можно найти в каталоге `notebooks`.

## Тестирование

Для тестирования используйте:

```
pytest -v
```

## Jupyter Notebooks

В репозитории присутствуют Jupyter notebooks. Откройте их, чтобы узнать больше о работе проекта и различных экспериментах.

## Изменения

Посмотрите [CHANGELOG.md](CHANGELOG.md) для истории изменений.

## Project Structure

```
├── repository/
    ├── CHANGELOG.md
    ├── Dockerfile
    ├── pyproject.toml
    ├── README.md
    ├── requirements.txt
    ├── notebooks/
        ├── test_Agent_RandomAgent.ipynb
        ├── quantstats-report.html
        ├── data/
    ├── tests/
        ├── stock_data_sample.csv
        ├── test_crossval.py
        ├── test_trading_env.py
        ├── test_agents.py
    ├── tradegym/
        ├── crossval.py
        ├── tradegym.py
        ├── __init__.py
        ├── rewards.py
        ├── agents/
            ├── baseline.py
            ├── sb3.py
            ├── __init__.py
```
