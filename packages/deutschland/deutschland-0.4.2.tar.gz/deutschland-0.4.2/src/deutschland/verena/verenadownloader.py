import math
from typing import List, Tuple

import requests
from bs4 import BeautifulSoup


class VerenaDownloader:
    """
    Downloads all pages (each containing 100 job offerings) of the VERENA portal.
    """

    BASE_URL = "https://www.schulministerium.nrw.de"

    def __init__(self):
        self.session = requests.Session()

    def __scrape_landing_page(self) -> Tuple[int, str, str]:
        """Returns (job_openings_count: int, access_listing_url_part: str, access_listing_action_id: str)

        Example: (513, "/BiPo/Verena/angebote?action=595.1764087184088", "595.1764087184088")

        Scrapes the VERENA landing page to get a session cookie, matching actionid
        to access the listing view and the count of job offerings in the listing.
        """
        landing_url = self.BASE_URL + "/BiPo/Verena"
        landing_request = self.session.get(landing_url)
        landing_soup = BeautifulSoup(landing_request.text, "html.parser")
        links = landing_soup.findAll("a", {"title": "Zu den Stellenausschreibungen"})
        for link in links:
            if "Derzeit im Netz verÃ¶ffentlichte Ausschreibungen:" in link.text:
                job_openings_count = link.find_next("strong").text
                access_listing_url_part = link["href"]
                # split action_id from listing_url_part
                access_listing_action_id = access_listing_url_part.replace(
                    "/BiPo/Verena/angebote?action=", ""
                )
                return (
                    int(job_openings_count),
                    access_listing_url_part,
                    access_listing_action_id,
                )

    def __scrape_listing_page_initial(
        self, access_listing_url_part: str
    ) -> Tuple[str, str, str]:
        """Returns (listing url with new actionid, blocksize 100 & valid suchid (aka. select_blocksize_url_part)), search_id, select_blocksize_action_id)

        Example: ("/BiPo/Verena/angebote?action=509.9848906326322&block=b100&suchid=188736", "188736", "509.9848906326322")

        Scrapes the VERENA listing page to get a listing url with blocksize = 100 and valid suchid (search_id).
        suchid is generated by the backend and stores your search preferences.
        """
        listing_url = self.BASE_URL + access_listing_url_part
        listing_request = self.session.get(listing_url)
        listing_soup = BeautifulSoup(listing_request.text, "html.parser")
        blocksize_selector = listing_soup.find("div", id="blockauswahl")
        # -1 is blocksize 100, also gets a such_id (search_id)
        select_blocksize_url_part = blocksize_selector.findAll("a")[-1]["href"]
        search_id = select_blocksize_url_part.split("=")[-1]
        select_blocksize_action_id = select_blocksize_url_part.replace(
            "/BiPo/Verena/angebote?action=", ""
        ).split("&")[0]
        return select_blocksize_url_part, search_id, select_blocksize_action_id

    def __set_block_size(self, select_blocksize_url_part: str):
        """
        Run GET on search ID url to set correct block size for future requests in backend
        """
        searchid_url = self.BASE_URL + select_blocksize_url_part
        self.session.get(searchid_url)

    def __generate_all_listing_urls(
        self, action_id: str, search_id: str, opening_count: int
    ) -> List[str]:
        """Based on action_id, search_id and opening_count, generates a list of all listing urls.

        Example: [
            "https://www.schulministerium.nrw.de/BiPo/Verena/angebote?action=901.7040712715743&seite=a1&suchid=188265",
            "https://www.schulministerium.nrw.de/BiPo/Verena/angebote?action=901.7040712715743&seite=a2&suchid=188265"
            ...
            ]
        """
        all_urls = []
        # because block size = 100
        site_count = math.ceil(opening_count / 100)
        for curr_site in range(0, site_count):
            curr_site += 1
            listing_format_string = (
                self.BASE_URL + "/BiPo/Verena/angebote?action={0}&seite=a{1}&suchid={2}"
            )
            all_urls.append(
                listing_format_string.format(action_id, curr_site, search_id)
            )
        return all_urls

    def __scrape_actual_listing(self, urls: List[str]):
        """Downloads the job listing pages provided by 'urls' and returns their content as an list of sourcecodes.

        Example: [
            <html>...</html>
            <html>...</html>
        ]

        """
        scraped_pages = []
        for url in urls:
            r = self.session.get(url)
            scraped_pages.append(r.text)
        return scraped_pages

    def scrape(self) -> List[str]:
        """Returns list of sourcecodes of all listing pages of the VERENA job listing portal.

        Example: [
            <html>...</html>
            <html>...</html>
        ]

        """
        (
            job_opening_count,
            access_listing_url_part,
            access_listing_action_id,
        ) = self.__scrape_landing_page()
        # select_blocksize_action_id is the action_id used to select the blocksize.
        # Its also reused to query the different pages of the job portal.
        (
            select_blocksize_url_part,
            search_id,
            select_blocksize_action_id,
        ) = self.__scrape_listing_page_initial(access_listing_url_part)
        self.__set_block_size(select_blocksize_url_part)
        all_listing_urls = self.__generate_all_listing_urls(
            select_blocksize_action_id, search_id, job_opening_count
        )
        return self.__scrape_actual_listing(all_listing_urls)


if __name__ == "__main__":
    vd = VerenaDownloader()
    res = vd.scrape()
    print(res)
