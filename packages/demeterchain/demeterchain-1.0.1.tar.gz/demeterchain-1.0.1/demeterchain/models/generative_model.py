import torch
import logging
from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig, BitsAndBytesConfig
from tqdm import trange
from abc import ABC, abstractmethod
from typing import Optional, List, Dict
from demeterchain.models import BaseModel
from demeterchain.data_modules import ContextPrefixTree
from demeterchain.utils import PromptTemplate, QAModelConfig, Document

logger = logging.getLogger(__name__)


class GenerativeModel(BaseModel):
    """
    A QA model that answers questions based on provided documents. 
    To reduce the illusion, the answer must come from the documents,
    Irrelevant documents will be filtered out when answering

    Examples:

        .. code-block:: python

            from demeterchain.models import GenerativeModel
            from demeterchain.utils import QAModelConfig, PromptTemplate

            # Load model
            model_config = QAModelConfig(
                model_name_or_path = "/path/to/model",
                noanswer_str = "無法回答",
                template = PromptTemplate(
                    input_variables = ["doc", "query"],
                    template="[INST] <<SYS>>\n請根據提供的問題，從提供的內文中尋找答案並回答，回答時只需要輸出答案，不需輸出其他資訊，如果從提供的內文無法找到答案，請回答\"無法回答\"\n<</SYS>>\n\n問題:\n{query}\n\n內文:\n{doc}\n [/INST]答案:\n"
                )
            )
            reader = GenerativeModel(config=model_config)

            # get answer
            inputs = {"query": "什麼水果是紅色的?"}
            docs = [
                "蘋果是紅色的水果",
                "香蕉是黃色的水果",
                "我是小明，今天天氣很好",
                "草莓不是黑的，是紅色的水果"]
            answer_doc = reader.get_answer(inputs, docs)

        >>> print(answer_doc)
        {'蘋果': '蘋果是紅色的水果', '草莓': '草莓不是黑的，是紅色的水果'}
    """
    DOCUMENT_VARIABLE = "doc"
    QUERY_VARIABLE = "query"
    ALLOWED_ANSWER_STRATEGY = ["best", "longest"]

    def __init__(self, config: QAModelConfig):
        """
        Args:
            config (QAModelConfig): Model settings. If both noanswer_str and noanswer_ids are none, the function of filtering irrelevant documents will be turned off.
        """
        model_kargs ={
            "device_map" : config.device_map,
            "torch_dtype" : config.dtype,
            "cache_dir" : config.cache_dir
        }

        if config.use_flash_attention:
            model_kargs["attn_implementation"] = "flash_attention_2"

        if config.quantize == None:
            model_kargs["torch_dtype"] = config.dtype
        elif config.quantize == "bitsandbytes":
            model_kargs["torch_dtype"] = config.dtype
            model_kargs["quantization_config"]=BitsAndBytesConfig(
                load_in_8bit=True,
            )
        elif config.quantize == "bitsandbytes-nf4":
            model_kargs["quantization_config"]=BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_compute_dtype=config.dtype,
            )
        else:
            raise ValueError(
                f"{config.quantize} is not valid quantize type"
            )

        if self.DOCUMENT_VARIABLE not in config.template.variables or self.QUERY_VARIABLE not in config.template.variables:
            raise ValueError(
                f"template must contain {{{self.DOCUMENT_VARIABLE}}} and {{{self.QUERY_VARIABLE}}}"
            )
        self.template = config.template

        self.tokenizer = AutoTokenizer.from_pretrained(config.model_name_or_path)
        self.tokenizer.padding_side = "left"
        if self.tokenizer.pad_token == None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

        self.model = AutoModelForCausalLM.from_pretrained(
            config.model_name_or_path,
            **model_kargs
        )
        self.compute_noanswer_ids(config.noanswer_str, config.noanswer_ids)
        super().__init__()

    def compute_noanswer_ids(self, noanswer_str: Optional[str] = None, noanswer_ids: Optional[List[int]] = None):
        """
        Tokenizes noanswer_str using self.tokenizer.

        Args:
            noanswer_str (Optional[str], defaults to `None`): String which let the model decide whether to ignore the article. 
            noanswer_ids (Optional[List[int]], defaults to `None`): tokenized String which let the model decide whether to ignore the document.
                The model will refer to noanswer_ids when generating text.
                There will be the following three results according to the provided parameters:
                - Provide noanswer_ids: noanswer_str will be generated by tokenizer.decode(noanswer_ids).
                - Only provide noanswer_str: noanswer_ids will be generated by noanswer_str.
                - both noanswer_str and noanswer_ids are None: irrelevant documents will not be ignored. Not recommended
        """
        self.use_noanswer_filter =True
        if noanswer_ids != None:
            self.noanswer_ids = noanswer_ids.copy()
            self.noanswer_str = self.tokenizer.decode(noanswer_ids)
        elif noanswer_str != None:
            start = len(self.template.template)
            template_ids = self.tokenizer(self.template.template + noanswer_str, return_offsets_mapping=True, return_tensors="pt")
            offset_mapping = template_ids.offset_mapping
            start_id = (offset_mapping[:,:,0].squeeze() >= start).nonzero()[0].item()

            self.noanswer_ids = template_ids.input_ids.squeeze()[start_id:].tolist()
            self.noanswer_str = noanswer_str
        else:
            logger.warning(
                f"both noanswer_str and noanswer_ids are None, irrelevant documents will not be ignored"
            )
            self.noanswer_ids = None
            self.noanswer_str = None
            self.use_noanswer_filter = False

    def get_answer(
        self, 
        inputs: Dict[str, str],
        docs: List[Document],
        batch_size: int = 1, 
        max_length: int = 768,
        max_new_tokens: int = 32,
        num_beams: int = 3,
        num_return_sequences: int = 3,
        answer_strategy: str = "best",
    ) -> List[str]:
        """
        Get answers from each provided document and ignore irrelevant documents

        Args:
            inputs (Dict[str, str]): Parameters to be put in self.template, ex : {"query": "什麼水果是紅色的?"}.
            docs (List[Document]): relevant documents retrieved by retriever.
            batch_size (int): Number of articles processed simultaneously. It has no impact on the prediction results. Please adjust according to your own GPU capabilities.
            max_length (int): The upper limit of tokens read by the model.
            max_new_tokens(int): The upper limit of tokens generated by the model.
            num_beams (int): Number of beams for beam search. 1 means no beam search.
            num_return_sequences (int): The number of independently computed returned sequences for each element in the batch.
            answer_strategy (str): How to select from multiple answers generated by a model.
        """
        extractive = True # It will be possible to make it a configurable parameter in subsequent versions.

        if answer_strategy not in self.ALLOWED_ANSWER_STRATEGY:
            raise ValueError(
                f"answer_strategy must be one of {self.ALLOWED_ANSWER_STRATEGY}"
            ) 

        doc_prefix, doc_postfix = self.template.split_format(inputs, split_variable="doc")

        doc_start = len(doc_prefix)

        # check template length is smaller than max_length
        template_len = self.tokenizer(
            doc_prefix, 
            doc_postfix, 
            add_special_tokens=False, 
            return_offsets_mapping=True,
            return_tensors="pt"
        ).input_ids.size(1)

        if template_len >= max_length:
            raise ValueError(
                "The template or query is too long. Please increase max_length or reduce the length of the template or query"
            ) 

        # ignore empty document and concate doc_prefix and doc
        non_empty_docs = list(filter(lambda doc:len(doc.page_content)>0, docs))
        doc_contents = [doc.page_content for doc in non_empty_docs]
        prompts = [doc_prefix + doc for doc in doc_contents]
        doc_postfix = [doc_postfix] * len(prompts)

        # Initialize prefix_tree
        context_prefix_tree =  ContextPrefixTree(eos_token_id = self.tokenizer.eos_token_id)

        # batch inference
        if self.use_noanswer_filter:
            unable_to_answer_substring = [self.noanswer_str[:i+1] for i in range(len(self.noanswer_str))]
        generation_config = GenerationConfig(
            max_new_tokens=max_new_tokens,
            num_beams = num_beams,
            num_return_sequences = num_return_sequences,
            pad_token_id = self.tokenizer.pad_token_id
        )
        answer_docs = {}
        for i in trange(0, len(prompts), batch_size):
            inputs = self.tokenizer(
                prompts[i:i+batch_size], 
                doc_postfix[i:i+batch_size],
                padding="max_length",
                truncation="only_first", 
                max_length = max_length,
                add_special_tokens=False,
                return_offsets_mapping=True,
                return_tensors="pt"
            ).to(self.model.device)
            context_prefix_tree.generate_prefix_tree(inputs, doc_start)
            if self.use_noanswer_filter:
                context_prefix_tree.add_string_to_prefix_tree(self.noanswer_ids)

            input_ids = inputs['input_ids']
            attention_mask = inputs['attention_mask']
            with torch.no_grad():
                outputs = self.model.generate(input_ids=input_ids,
                    attention_mask=attention_mask,
                    generation_config=generation_config,
                    repetition_penalty=1.0,
                    prefix_allowed_tokens_fn= None if not extractive else context_prefix_tree.find_allowed_tokens
                )
            generate_answers = self.tokenizer.batch_decode(outputs[:,inputs.input_ids.size(1):], skip_special_tokens=True)

            # separate answers from different docs
            for j in range(0, len(generate_answers), num_return_sequences):
                if self.use_noanswer_filter:
                     # remove all answer from the same document if one of the answer is noanswer_str.
                    if any([answer.replace(" ", "") in unable_to_answer_substring for answer in generate_answers[j:j+num_return_sequences]]):
                        continue

                if answer_strategy == "best":
                    answer = generate_answers[j]
                elif answer_strategy == "longest":
                    answers_len = [len(answer) for answer in generate_answers[j:j+num_return_sequences]]
                    longest_answer_idx = answers_len.index(max(answers_len))
                    answer = generate_answers[j + longest_answer_idx]
                answer = (answer.replace(" ", "")).replace("。", "")
                # not choose an existing answer
                if answer not in answer_docs:
                    answer_docs[answer] = non_empty_docs[i + j//num_return_sequences]
        return answer_docs