{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ab1d0a7-5931-4123-ad77-2f9504523822",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "930d7451-81aa-40ef-9e0f-9dc13a18d687",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src/ml_indie_tools')  # Point to local module source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a28207b-559f-4530-b647-1a06c82d8df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(encoding='utf-8', level=logging.INFO)\n",
    "from Gutenberg_Dataset import Gutenberg_Dataset\n",
    "from Text_Dataset import Text_Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26e09695-1952-4464-a3f8-0824ee64d48b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The discovery and decipherment of the trilingual cuneiform inscriptions\n",
      "Prolegomena to the Study of Hegel's Philosophy\n",
      "Kant's Prolegomena\n",
      "The Cornish Fishermen's Watch Night and Other Stories\n",
      "Prolegomena to the History of Israel\n",
      "Legge Prolegomena\n"
     ]
    }
   ],
   "source": [
    "gd=Gutenberg_Dataset()\n",
    "gd.load_index()\n",
    "bl=gd.search({'title': ['proleg', 'hermen'], 'language': ['english']})\n",
    "bl=gd.insert_book_texts(bl)\n",
    "for i in range(len(bl)):\n",
    "    print(bl[i]['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07ccb1c1-8073-436b-a5e0-69cf7c5725f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Datasets:Loaded 6 texts\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "If we writ<span style=\"background-color:#edebd0;\">e anything t</span><sup>[5]</sup><span style=\"background-color:#d8daef;\">hat contains </span><sup>[1]</sup><span style=\"background-color:#edebd0;\">parts of the s</span><sup>[5]</sup>ources, like<span style=\"background-color:#d8daef;\">: that is t</span><sup>[1]</sup><span style=\"background-color:#edebd0;\">heir motto</span><sup>[5]</sup>, then a highligh<span style=\"background-color:#ebdef0;\">t will be a</span><sup>[2]</sup>pplied."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#edebd0;\">Julius Wellhausen: Prolegomena to the History of Israel</span><sup>[5]</sup>, <span style=\"background-color:#d8daef;\">Arthur John Booth: The discovery and decipherment of the trilingual cuneiform inscriptions</span><sup>[1]</sup>, <span style=\"background-color:#ebdef0;\">William Wallace and G. W. F. Hegel: Prolegomena to the Study of Hegel's Philosophy</span><sup>[2]</sup></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tl = Text_Dataset(bl)  # bl contains a list of texts (books from Gutenberg)\n",
    "tl.source_highlight(\"If we write anything that contains parts of the sources, like: that is their motto, then a highlight will be applied.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "840ea739-0f24-4598-8b0d-d77a4e167c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Datasets:Starting tokenizer on 6 texts...\n",
      "INFO:Datasets:Extracting bytegrams of length 1..5 from text_list, selecting 5000 (- 256 for single bytes) most used ngrams.\n",
      "INFO:Datasets:Corpus from byte texts created.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text length 81, That would be a valid argument if we hadn't defeated it's assumptions way before.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Datasets:Larger bytegrams calculated: The discovery and decipherment of the trilingual cuneiform inscriptions: 911566, dict: 216453\n",
      "INFO:Datasets:Larger bytegrams calculated: Prolegomena to the Study of Hegel's Philosophy: 857839, dict: 306524\n",
      "INFO:Datasets:Larger bytegrams calculated: Prolegomena to the History of Israel: 1440900, dict: 442573\n",
      "INFO:Datasets:weights compiled\n",
      "INFO:Datasets:Removed 161 bytegrams of length 1 from bytegrams_list: they shouldn't be there. (XXX)\n",
      "INFO:Datasets:Encoding text corpora\n",
      "INFO:Datasets:Encoding larger text The discovery and decipherment of the trilingual cuneiform inscriptions...\n",
      "INFO:Datasets:Encoding larger text Prolegomena to the Study of Hegel's Philosophy...\n",
      "INFO:Datasets:Encoding larger text Prolegomena to the History of Israel...\n",
      "INFO:Datasets:Encoding text corpora done.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token-count: 33, [697, 97, 816, 970, 98, 2462, 118, 1331, 615, 3193, 117, 1275, 3385, 4809, 280, 110, 39, 2589, 3380, 97, 3850, 116, 2797, 97, 538, 117, 795, 116, 847, 4098, 98, 1297, 46]\n"
     ]
    }
   ],
   "source": [
    "test_text=\"That would be a valid argument if we hadn't defeated it's assumptions way before.\"\n",
    "print(f\"Text length {len(test_text)}, {test_text}\")\n",
    "tokenizer='bytegram'\n",
    "tl.init_tokenizer(tokenizer=tokenizer)\n",
    "st = tl.tokenize(test_text)\n",
    "print(f\"Token-count: {len(st)}, {st}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6628575-15f3-40c0-815a-85e475219898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tl.get_unique_token_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6d6f0ea-92b8-48be-be59-f07ae3d6cbb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text length 84, √∞∆í That would be a valid argument if we hadn't defeated it's assumptions way before.\n",
      "Token-count: 37, [195, 176, 198, 146, 640, 97, 816, 970, 98, 2462, 118, 1331, 615, 3193, 117, 1275, 3385, 4809, 280, 110, 39, 2589, 3380, 97, 3850, 116, 2797, 97, 538, 117, 795, 116, 847, 4098, 98, 1297, 46]\n",
      "√∞∆í That would be a valid argument if we hadn't defeated it's assumptions way before.\n"
     ]
    }
   ],
   "source": [
    "test2=\"√∞∆í \"+test_text\n",
    "print(f\"Text length {len(test2)}, {test2}\")\n",
    "el=tl.encode(test2)\n",
    "print(f\"Token-count: {len(el)}, {el}\")\n",
    "dl = tl.decode(el)\n",
    "print(dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bb8947f-e546-4a62-9586-14baa44cff80",
   "metadata": {},
   "outputs": [],
   "source": [
    "tl.init_getitem(\"encoded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b4ad9bc-b468-4bfe-b9b4-b8d89fc708bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'that can render cuneiform characters such as êé†, êé°, êé¢. If these do\\nnot display for you, then one suitable option is the font ‚ÄòSegoe ui\\nhistoricÔøΩÔøΩÔøΩ. Cuneiform signs'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tl.decode(tl[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a837763-652e-4fb9-bdb1-0c51c9e29b24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tl.get_unique_token_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98659664-ffc0-46bb-a0f7-104dde2e7b26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bytearray(b'\\xc3\\xb0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bytearray('√∞', 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8099ef4-e26b-461d-b00b-dbd6a7735782",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
